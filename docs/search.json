[
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Health Economics II",
    "section": "Instructor",
    "text": "Instructor\n Ian McCarthy  R. Rollins, R418  ian.mccarthy@emory.edu  @ianhealthecon  Schedule an appointment"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "Health Economics II",
    "section": "Course Details",
    "text": "Course Details\n 1-13-2026 to 4-23-2026  Tuesday, Thursday  10:00am to 11:15am  Ignatius Few 129"
  },
  {
    "objectID": "schedule/4-literature.html",
    "href": "schedule/4-literature.html",
    "title": "Hospital Competition and Pricing",
    "section": "",
    "text": "Module 4 examines competition and pricing in hospital markets. We study how market structure, bargaining relationships, and organizational form shape hospital prices, quality, and strategic behavior. The readings span foundational work on hospital market power and price determination, reduced-form evidence on the effects of hospital mergers, and structural models of insurer–hospital bargaining used to evaluate counterfactual market outcomes. The module also covers recent research on cross-market mergers and vertical integration, highlighting how system-level organization and ownership arrangements alter competitive incentives beyond local markets. Together, these papers emphasize the central role of market power and contracting institutions in shaping prices and welfare in healthcare markets, and provide a bridge between physician behavior and insurer–provider interactions later in the course. Below is a drop-down list of papers most relevant for this module.\n\n    List of Key Papers (click to expand)"
  },
  {
    "objectID": "schedule/2-literature.html",
    "href": "schedule/2-literature.html",
    "title": "Healthcare Variation and Physician Agency",
    "section": "",
    "text": "In this module, we study how physicians make treatment decisions in environments characterized by asymmetric information, discretion, and multiple, sometimes conflicting objectives. We begin by introducing physician agency as a central organizing framework for understanding medical decision-making, and then examine how both financial and non-financial incentives shape physician behavior. We conclude by moving beyond the individual physician to consider how treatment decisions are influenced by organizational structure and team-based care, highlighting how agency operates within groups, hospitals, and integrated delivery systems. Here’s a long list of papers that I think are particularly relevant in this area.\n\n    List of Key Papers (click to expand)"
  },
  {
    "objectID": "assignments/exercise2.html",
    "href": "assignments/exercise2.html",
    "title": "Exercise 2: Instrumental Variables",
    "section": "",
    "text": "Overview\nIn this assignment, we’re going to work through some applied issues related to instrumental variables. For a long time, IV (or 2SLS) was a very common identification strategy for applied empirical micro, but it fell out of favor as people became more aware of the assumptions underlying the estimator and better understood what IV actually estimates (not the ATE in most cases). People also started to find other strategies that were more compelling in some applications (and of course with some other assumptions). In this assignment, we’re going to study the effects of a physician’s affiliation with a hospital on physician practice patterns, and we’ll instrument for physician affiliation using some specific Medicare payment shocks.\n\n\nResources and data\nThe data for this assignment comes from three sources:\n\nMD-PPAS; The Medicare Data on Provider Practice and Specialty includes data on physician specialties, practice IDs, demographics, and place of service. Be sure to follow the link and read the data documentation. We’ll use these data to construct a measure of physician integration.\nMedicare Utilization and Payment Data: These files provide data on the quantities and Medicare spending of each physician and service. We’ll use these data to capture total physician-level billing activity, and we’ll use the service-level data to measure the revenue effects from our plausibly exogenous policy shock. These data are only available beginning in 2012. These files are large but otherwise relatively clean and easy to use, so there’s no separate repo for these data. Note that we will only work with data for MDs, so you can drop a lot of observations with that restriction.\nPhysician Fee Schedule 2010 Update: Our instrument mainly consists of a shock to physician payments introduced in 2010. The shock further increased payments for services in an outpatient facility compared to services billed in a physician’s office. The GitHub repo (linked above) provides code to recreate a dataset with service-specific price shocks introduced by the 2010 fee schedule update. To save us some time, I’ve posed the final dataset from that repo into our class data folder.\n\n\n\nQuestions\nIn your GitHub repository, please be sure to clearly address/answer the following questions. Note that our utilization and payment data only start in 2012 (just a limitation of using publicly available data), while the price shock first takes place in 2010. Thankfully, the price shock was introduced gradually from 2010 through 2013, so our instrument (see question 4 below) still has some variation over our time period. For all of this analysis, please focus only on the years from 2012 through 2017 and focus only on MDs (based on the “NPPES Credentials” field in the PUF data). Please use the raw values for your summary statistics, but you might consider using logs in the rest of the analysis due to the heavily skewed nature of the data.\n\nProvide and discuss a table of simple summary statistics showing the mean, standard deviation, min, and max of total physician-level Medicare spending, claims, and patients. Use the Medicare utilization and payment data to calculate total spending, claims, and patients at the physician level. You can do this using the average Medicare allowed amt * bene_day_srvc_cnt (or Medicare allowed amt * line_srvc_cnt) for spending, bene_day_srvc_cnt or the line_srvc_cnt for claims, and bene_unique_cnt for patients. The patient counts will include some overlap since the data are by service, but that’s OK for our purposes.\nForm a proxy for integration using the ratio: \\[\\begin{equation}\nINT_{it} = \\mathbf{1} \\left(\\frac{HOPD_{it}}{HOPD_{it} + OFFICE_{it} + ASC_{it}} \\geq 0.75\\right),\n(\\#eq:int)\n\\end{equation}\\] where \\(HOPD_{it}\\) reflects the total number of claims in which physician \\(i\\) bills in a hospital outpatient setting, \\(OFFICE_{it}\\) is the total number of claims billed to an office setting, and \\(ASC_{it}\\) is the total number of claims billed to an ambulatory surgery center. As reflected in Equation @ref(eq:int), you can assume that any physician with at least 75% of claims billed in an outpatient setting is integrated with a hospital. Using this 75% threshold, plot the mean of total physician-level claims for integrated versus non-integrated physicians over time.\nEstimate the relationship between integration on (log) total physician claims using OLS, with the following specification: \\[\\begin{equation}\ny_{it} = \\delta INT_{it} + \\beta x_{it} + \\gamma_{i} + \\gamma_{t} + \\varepsilon_{it},\n(\\#eq:ols)\n\\end{equation}\\] where \\(INT_{it}\\) is defined in Equation @ref(eq:int), \\(x_{it}\\) captures time-varying physician characteristics, and \\(\\gamma_{i}\\) and \\(\\gamma_{t}\\) denote physician and time fixed effects. Please focus on physician’s that weren’t yet integrated as of 2012, that way we have some pre-integration data for everyone. Impose this restriction for the remaining questions. Feel free to experiment with different covariates in \\(x_{it}\\) or simply omit that term and only include the fixed effects.\nHow much should we be “worried” about endogeneity here? Extending the work of Altonji, Elder, and Taber (2005), Oster (2019) derives the expression \\[\\begin{equation}\n\\delta^{*} \\approx \\hat{\\delta}_{D,x_{1}} - \\rho \\times \\left[\\hat{\\delta}_{D} - \\hat{\\delta}_{D,x_{1}}\\right] \\times \\frac{R_{max}^{2} - R_{D,x_{1}}^{2}}{R_{D,x_{1}}^{2} - R_{D}^{2}} \\xrightarrow{p} \\delta,\n(\\#eq:oster)\n\\end{equation}\\] where \\(x_{1}\\) captures our observable covariates (or fixed effects in our case); \\(\\delta\\) denotes the treatment effect of interest; \\(\\hat{\\delta}_{D,x_{1}}\\) denotes the coefficient on \\(D\\) from a regression of \\(y\\) on \\(D\\) and \\(x_{1}\\); \\(R_{D,x_{1}}^{2}\\) denotes the \\(R^{2}\\) from that regression; \\(\\hat{\\delta}_{D}\\) denotes the coefficient on \\(D\\) from a regression of \\(y\\) on \\(D\\) only; \\(R_{D}^{2}\\) reflects the \\(R^{2}\\) from that regression; \\(R_{max}^{2}\\) denotes an unobserved “maximum” \\(R^{2}\\) from a regression of \\(y\\) on \\(D\\), observed covariates \\(x_{1}\\), and some unobserved covariates \\(x_{2}\\); and \\(\\rho\\) denotes the degree of selection on observed variables relative to unobserved variables. One approach that Oster suggests is to consider a range of \\(R^{2}_{max}\\) and \\(\\rho\\) to bound the estimated treatment effect, where the bounds are given by \\(\\left[ \\hat{\\delta}_{D,x_{1}}, \\delta^{*}(R^{2}_{max}, \\rho) \\right]\\). Construct these bounds based on all combinations of \\(\\rho \\in (0, .5, 1, 1.5, 2)\\) and \\(R_{max}^{2} \\in (0.5, 0.6, 0.7, 0.8, 0.9, 1)\\) and present your results in a table. What do your results say about the extent to which selection on observables could be problematic here? Hint: you can also look into psacalc in Stata or robomit in R for implementation of Oster (2019) in Stata or R, respectively.\nConstruct the change in Medicare payments achievable for an integrated versus non-integrated physician practice due to the 2010 update to the physician fee schedule, \\(\\Delta P_{it}\\). Use this as an instrument for \\(INT_{it}\\) in a 2SLS estimator following the same specification as in Equation @ref(eq:ols). Present your results along with those of your “first stage” and “reduced form”.\n\nHere is a little code snippet to help you work with the fee schedule update and the utilization and payment data in constructing the instrument. In this code chunk, the medicare.puf object is the provider and utilization data for a specific year, the pfs.yearly object is the physician fee schedule update data for the same year (except for years after 2013, in which case pfs.yearly should just be the 2013 data because the price shock is fully implemented as of 2013), and the taxid.base object is the MD-PPAS data from 2009 limited to just the NPI and the group1 variable (the group1 and group2 variables are encrypted versions of the physician’s tax ID, and I use the 2009 data so that I get a baseline measure of the practice before the price shock takes effect). The purpose of this code is to first merge the price shock information into service-level quantity data, then construct the total increase in revenue from the price shock based on observed quantities (that’s the numer variable), and divide by the total hypothetical revenue if payments never changed. The resulting phy_rev_change is intended to measure the increase in revenue for a given physician relative to revenue without the price shock. Finally, I average this across all physicians in a practice based on their observed practice affiliation as of 2009 and multiply by the practice size (I really just sum the ratio, but that’s the same thing). The resulting practice_rev_change variable is what you should use as your instrument for \\(INT_{it}\\).\n\n  price.shock &lt;- medicare.puf %&gt;% inner_join(taxid.base, by=\"npi\") %&gt;%\n    inner_join(pfs.yearly %&gt;% \n                 select(hcpcs, dprice_rel_2010, price_nonfac_orig_2010, price_nonfac_orig_2007), \n               by=c(\"hcpcs_code\"=\"hcpcs\")) %&gt;%\n    mutate_at(vars(dprice_rel_2010, price_nonfac_orig_2010, price_nonfac_orig_2007), replace_na, 0) %&gt;%\n    mutate(price_shock = case_when(\n            i&lt;=2013 ~ ((i-2009)/4)*dprice_rel_2010,\n            i&gt;2013  ~ dprice_rel_2010),\n          denom = line_srvc_cnt*price_nonfac_orig_2010,\n          numer = price_shock*line_srvc_cnt*price_nonfac_orig_2010) %&gt;%\n    group_by(npi) %&gt;%\n    summarize(phy_numer=sum(numer, na.rm=TRUE), phy_denom=sum(denom, na.rm=TRUE), tax_id=first(tax_id)) %&gt;%\n    ungroup() %&gt;%\n    mutate(phy_rev_change=phy_numer/phy_denom) %&gt;%    \n    group_by(tax_id) %&gt;%\n    summarize(practice_rev_change=sum(phy_rev_change, na.rm=TRUE)) %&gt;%\n    ungroup()\n\nYes, the idea of summing a ratio is a bit odd. But it’s easier to think of the instrument as the product of baseline (pre-shock) practice size and the average relative revenue change due to the price shock. In that context, the sum of the ratio is really just an interaction term that incorporates information on the price shock magnitude and baseline practice size. Each of these things alone are poor instruments, but together for the practice it reflects a “better” instrument.\n\nAssess the “need” for IV by implementing a Durbin-Wu-Hausman test with an augmented regression. Do this by first estimating the regression, \\(INT_{it} = \\lambda \\Delta P_{it} + \\beta x_{it} + \\gamma_{i} + \\gamma_{t} + \\varepsilon_{it}\\), take the residual \\(\\hat{\\nu} = INT_{it} - \\hat{INT}_{it}\\), and run the regression \\[y_{it} = \\delta INT_{it} + \\beta x_{it} + \\gamma_{i} + \\gamma_{t} + \\kappa \\hat{\\nu} + \\varepsilon_{it}.\\] Discuss your results for \\(\\hat{\\kappa}\\).\nNow let’s pay attention to potential issues of weak instruments. As we discussed in class, one issue with weak instruments is that our typical critical values (say, 1.96 for a 95% confidence interval) from the equation of interest (sometimes called the structural equation) are too low in the presence of a weak first-stage. These issues are presented very clearly and more formally in the Andrews, Stock, and Sun (2019) survey article. For this question, you will consider two forms of inference in the presence of weak instruments. Hint: Check out the ivmodel package in R or the ivreg2 command in Stata for help getting the AR Wald statistic.\n\nPresent the results of a test of the null, \\(H_{0}: \\delta=0\\), using the Anderson-Rubin Wald statistic. Do your conclusions from this test differ from a traditional t-test following 2SLS estimation of Equation @ref(eq:ols)?\nGoing back to your 2SLS results…inflate your 2SLS standard errors to form the \\(tF\\) adjusted standard error, following Table 3 in Lee et al. (2021). Repeat the test of the null, \\(H_{0}: \\delta=0\\), using standard critical values and the \\(tF\\) adjusted standard error.\n\nFollowing the Borusyak and Hull (2021) working paper (BH), we can consider our instrument as a function of some exogenous policy shocks and some possibly endogenous physician characteristics, \\(\\Delta P_{it}=f\\left(g_{pt}; z_{ipt}\\right)\\), where \\(g_{pt}\\) captures overall payment shocks for procedure \\(p\\) at time \\(t\\), and \\(z_{ip}\\) denotes a physician’s quantity of different procedures at baseline. We can implement the BH re-centering approach as follows:\n\nConsider hypothetical price changes over a set of possible counterfactuals by assuming that the counterfactuals consist of different allocations of the observed relative price changes. For example, take the vector of all relative price changes, reallocate this vector randomly, and assign new hypothetical relative price changes. Do this 100 times. This isn’t “all” possible counterfactuals by any means, but it will be fine for our purposes.\nConstruct the expected revenue change over all possible realizations from previously, \\(\\mu_{it} = E [\\Delta P_{it}]= \\sum_{s=1}^{100} \\sum_{p} g_{pt}^{s} z_{ip}\\).\nRe-estimate Equation @ref(eq:ols) by 2SLS when instrumenting for \\(INT_{it}\\) with \\(\\tilde{\\Delta} P_{it} = \\Delta P_{it} - \\mu_{it}\\). Intuitively, this re-centering should isolate variation in the instrument that is only due to the policy and remove variation in our instrument that is due to physician practice styles (the latter of which is not a great instrument).\n\nDiscuss your findings and compare estimates from different estimators.\nReflect on this assignment. What did you find most challenging? What did you find most surprising?\n\n\n\n\n\n\n\n\n\nReferences\n\nAltonji, Joseph G, Todd E Elder, and Christopher R Taber. 2005. “An Evaluation of Instrumental Variable Strategies for Estimating the Effects of Catholic Schooling.” Journal of Human Resources 40 (4): 791–821.\n\n\nLee, David S., Justin McCrary, Marcelo J. Moreira, and Jack R. Porter. 2021. “Valid t-Ratio Inference for IV.” Working {Paper}. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w29124.\n\n\nOster, Emily. 2019. “Unobservable Selection and Coefficient Stability: Theory and Evidence.” Journal of Business & Economic Statistics 37 (2): 187–204. https://doi.org/10.1080/07350015.2016.1227711."
  },
  {
    "objectID": "schedule/proposals1.html",
    "href": "schedule/proposals1.html",
    "title": "Proposal Demonstration",
    "section": "",
    "text": "Today I’ll try to demonstrate a research proposal. Remember that the research proposal should be a new idea that you aren’t sure will actually work, but you have a general sense of what you’d like to do and why. Below is the text for my proposal, with slides available here."
  },
  {
    "objectID": "schedule/proposals1.html#research-question",
    "href": "schedule/proposals1.html#research-question",
    "title": "Proposal Demonstration",
    "section": "Research question",
    "text": "Research question\nMy research question is: how does decision assistance affect health care utilization and ultimately health outcomes? More specifically, I want to test whether decision assistance (e.g., availability of spouse/partner, nearby children or other family, close friends) reduces medical errors or otherwise changes the way individuals access health care, and I want to estimate the health effects of any such changes. A related question is if cognitive decline affects health care utilization, and whether decision assistance can mitigate these effects."
  },
  {
    "objectID": "schedule/proposals1.html#what-is-the-hook",
    "href": "schedule/proposals1.html#what-is-the-hook",
    "title": "Proposal Demonstration",
    "section": "What is the hook?",
    "text": "What is the hook?\nIn many settings, individuals rely on expert agents to make decisions on their behalf. This is particularly true in healthcare, where patients often rely on physicians to make decisions about their care. However, alignment between a physician’s treatment decisions and patient preferences may be limited by the physician’s knowledge, biases, or financial incentives. For example, physicians may be more likely to recommend treatments that are more profitable for them or their organization, more likely to recommend treatments for which they are more familiar, or more likely to discount certain symptoms or side-effects based on their biases. All such factors tend to drive a wedge between a patient’s preferences for care and the actual treatment decisions recommended by the physician. This proposal focuses on the role of decision assistance in reducing this wedge (i.e., reducing the role of non-clinical factors such as physician bias, financial incentives, or decision heuristics)."
  },
  {
    "objectID": "schedule/proposals1.html#economic-model",
    "href": "schedule/proposals1.html#economic-model",
    "title": "Proposal Demonstration",
    "section": "Economic model",
    "text": "Economic model\nAs a starting point, consider the model of physician agency in Cutler et al. (2019). In their model, patient \\(i\\) maximizes their utility over healthcare, \\(V=V(p, Y, h, \\eta)\\), where \\(p\\) denotes healthcare prices, \\(Y\\) denotes income, \\(h\\) denotes health, and \\(\\eta\\) generally captures preferences for care. A simplified example is to consider patient’s utility given by \\(V(x) = B_{D}(x) - px\\), where \\(B_{D}(x)\\) captures the patient’s perceived benefits of care, \\(x\\), and \\(p\\) denotes the patient’s out-of-pocket price. The patient’s optimum amount of care from maximizing this utility is denoted, \\(x^{D}\\).\nPhysician \\(j\\)’s utility when treating patient \\(i\\) is then given by\n\\[\nu_{ij} = \\alpha B_{S}(x_{i}; \\theta_{i}) + \\beta \\pi(x_{i}; \\theta_{i}),\n\\tag{1}\\]\nwhere \\(B_{S}(x_{i}; \\theta_{i})\\) captures the physician’s perceived benefits of care, \\(\\alpha\\) denotes the weight that a physician places on perceived benefits (i.e., physician’s altruism), \\(\\pi(x_{i}; \\theta_{i})\\) denotes the physician’s profit from providing care, and \\(\\beta\\) denotes the weight assigned to profit in the physician’s utility.\nIn Equation 1, \\(\\theta_{i}\\) captures the “accuracy” of a physician’s assessment of the patient’s needs. Denoting the true health benefits of care by \\(B(x)\\), and normalizing \\(\\theta_{i}\\) such that \\(\\theta_{i} \\in [0,1]\\), then \\(B_{S}(x_{i}; \\theta_{i}) \\rightarrow_{\\theta_{i} \\rightarrow 1} B(x_{i})\\). But as reflected in \\(\\pi(x_{i}; \\theta_{i})\\), increasing \\(\\theta_{i}\\) is costly to the physician. For example, this may require the physician to spend more time with the patient, or to consult with other physicians.\nFor the purposes of this proposal, the key is that \\(\\theta_{i}\\) is patient specific, so that it may be easier for a physician to assess the needs of some patients than others. Patients with more support from family or friends may be easier to assess, as such support can relay important information to the physician or advocate more strongly for certain tests, and thus \\(\\theta_{i}\\) may be higher for these patients. In maximizing Equation 1, the physician will select some amount of care, \\(x^{*}(\\theta_{i})\\), which may increase or decrease in \\(\\theta_{i}\\) depending on other elements of the model.\n\nOther frameworks\nThe model above is a starting point, but there are other frameworks that could be used to model the role of decision assistance. For example,\n\nDecision assistance as a productivity shifter: Cutler et al. (2019) consider a similar mechanism in the form of physician productivity shifters, where they denote perceived health benefits by \\(g(x)=\\alpha_{j} + s(x)\\). In their setup, \\(s(x)\\) is the true health benefit of care \\(x\\), and \\(\\alpha_{j}\\) denotes physician productivity, which the authors suggest could vary across patients due to professional uncertainty.\nDecision assistance and learning: Another possible modeling framework could come from the physician learning literature wherein physicians select treatment based on the perceived match value between the patient and the treatment (as in Crawford and Shum (2005)). This match value is a function of the physician’s knowledge of the patient, and the physician’s knowledge of the treatment. In this framework, decision assistance could be modeled as increasing the physician’s knowledge of the patient, and thus increasing the match value between the patient and the treatment and facilitating faster learning."
  },
  {
    "objectID": "schedule/proposals1.html#empirical-strategy",
    "href": "schedule/proposals1.html#empirical-strategy",
    "title": "Proposal Demonstration",
    "section": "Empirical strategy",
    "text": "Empirical strategy\nIdentifying the effect of decision assistance on healthcare utilization requires some exogenous shift in availability of decision assistance. Possible sources of such variation include the sudden loss or gain of a spouse or partner, loss or gain of nearby children or other family. I propose the Health and Retirement Study (HRS), linked to Medicare claims data, to identify such variation and its effects on health outcomes and utilization. The HRS is a longitudinal survey of individuals over the age of 50, and includes detailed information on health, healthcare utilization, and decision assistance.\nSimilar to the patient movers literature initiated by Finkelstein, Gentzkow, and Williams (2016), I propose to use the HRS to identify individuals who experience a sudden change in decision assistance due to the unexpected death of a spouse or partner, the move of close family (e.g., children moving away from or closer to parents), or an individual’s own move toward or away from family. I will then use Medicare claims data to identify changes in healthcare utilization and health outcomes following these changes in decision assistance."
  },
  {
    "objectID": "schedule/5-3.html",
    "href": "schedule/5-3.html",
    "title": "Price Transparency",
    "section": "",
    "text": "Unlike most goods markets, prices in healthcare are typically determined through negotiation rather than posted prices, and are often opaque to patients, providers, and even market participants themselves. Recent policy efforts aimed at increasing price transparency seek to reduce this opacity by making prices observable, with the goal of promoting competition and lowering spending. Whether transparency achieves these goals depends critically on how price information enters bargaining, demand, and strategic interaction.\nThe literature on price transparency emphasizes that disclosure can have ambiguous effects in markets with imperfect competition. Transparency may shift patient demand or referral patterns, but it can also alter bargaining positions, outside options, and negotiated prices between firms. As a result, transparency can intensify competition in some settings while facilitating coordination or redistributing surplus in others. Understanding these effects requires attention to equilibrium responses and institutional detail.\nWe introduce this literature by focusing on empirical work that studies how price information affects outcomes in bargaining and competitive environments. These papers highlight when transparency improves efficiency and when it may instead change who captures surplus without lowering prices. This class concludes the module by emphasizing transparency as a mechanism that reshapes strategic interaction, rather than as a uniformly pro-competitive policy.\nPotential papers for presentation today include:\n\nGrennan and Swanson (2020) — a canonical analysis of how transparency affects negotiated prices and bargaining power\nChristensen, Floyd, and Maffett (2020) — evidence on price transparency regulation and healthcare prices\nBrown (2019) — equilibrium effects of healthcare price information\n\n\n\n\n\nReferences\n\nBrown, Zach Y. 2019. “Equilibrium Effects of Health Care Price Information.” The Review of Economics and Statistics 101 (4): 699–712. https://doi.org/10.1162/rest_a_00765.\n\n\nChristensen, Hans B., Eric Floyd, and Mark Maffett. 2020. “The Only Prescription Is Transparency: The Effect of Charge-Price-Transparency Regulation on Healthcare Prices.” Management Science 66 (7): 2861–82. https://doi.org/10.1287/mnsc.2019.3330.\n\n\nGrennan, Matthew, and Ashley Swanson. 2020. “Transparency and Negotiated Prices: The Value of Information in Hospital-Supplier Bargaining.” Journal of Political Economy 128 (4): 1234–68."
  },
  {
    "objectID": "schedule/5-1.html",
    "href": "schedule/5-1.html",
    "title": "Physician Quality",
    "section": "",
    "text": "A longstanding challenge in healthcare markets is that provider quality is difficult for patients to observe prior to receiving care. Public reporting of physician outcomes and quality measures is intended to address this information problem by helping patients make better choices and by creating incentives for providers to improve performance. Understanding the effects of physician quality disclosure therefore requires examining both how patients respond to quality information and how providers adjust their behavior in response to increased observability.\nThe literature on physician quality disclosure emphasizes two key margins of adjustment. First, public reporting can shift patient demand and referral patterns toward higher-rated providers, potentially improving allocative efficiency. Second, disclosure can induce changes in provider behavior through reputational concerns, intrinsic motivation, or strategic responses such as selection and gaming. Importantly, these effects need not align, and quality disclosure may improve measured performance without improving underlying care.\nWe introduce this literature by focusing on empirical work that isolates the causal effects of physician quality reporting on patient choice and provider behavior. These studies highlight both the promise of disclosure as a policy tool and the challenges it poses for measurement and welfare analysis. This discussion sets up the following classes on insurance ratings and price transparency, where similar mechanisms operate in different institutional contexts.\nPotential papers for presentation today include:\n\nDranove et al. (2003) — a foundational study of how public report cards affect provider behavior and outcomes\nKolstad (2013) — evidence on physician responses to quality disclosure driven by intrinsic motivation\nEpstein (2010) — analysis of how quality report cards reshape referral patterns among physicians\n\n\n\n\n\nReferences\n\nDranove, David, Daniel Kessler, Mark McClellan, and Mark Satterthwaite. 2003. “Is More Information Better? The Effects of Report Cards on Health Care Providers.” Journal of Political Economy 111 (3): 555–88.\n\n\nEpstein, Andrew J. 2010. “Effects of Report Cards on Referral Patterns to Cardiac Surgeons.” Journal of Health Economics 29 (5): 718–31. https://doi.org/10.1016/j.jhealeco.2010.06.002.\n\n\nKolstad, Jonathan T. 2013. “Information and Quality When Motivation Is Intrinsic: Evidence from Surgeon Report Cards.” American Economic Review 103 (7): 2875–2910."
  },
  {
    "objectID": "schedule/4-4.html",
    "href": "schedule/4-4.html",
    "title": "Vertical Integration",
    "section": "",
    "text": "Competition in healthcare is often discussed in horizontal terms, but vertical integration between providers and across levels of the healthcare supply chain has become increasingly prevalent. Hospitals have expanded their ownership of physician practices, insurers have acquired provider organizations, and health systems have adopted integrated delivery models that combine financing and care provision. These arrangements can alter incentives, information flows, and bargaining relationships in ways that standard horizontal competition frameworks do not capture.\nThe literature on vertical integration in healthcare examines whether these organizational changes primarily generate efficiency gains—such as improved care coordination, better information sharing, and reduced transaction costs—or instead increase market power through foreclosure, higher negotiated prices, or changes in referral patterns. Empirical studies document that vertical integration can affect prices, utilization, and spending, but the mechanisms remain an active area of debate. In contrast to horizontal mergers, the competitive effects of vertical integration often depend on subtle institutional details, including referral incentives, billing rules, and contractual restrictions.\nWe introduce this literature by focusing on recent empirical work that studies hospital–physician and insurer–provider integration using quasi-experimental designs. These papers highlight how vertical integration reshapes competitive dynamics without necessarily changing traditional concentration measures, and they underscore the importance of organizational form for understanding pricing and welfare in healthcare markets.\nPotential papers for presentation today include:\n\nCuesta, Noton, and Vatter (2019) — evidence on hospital–physician integration and spending\nKoch, Wendling, and Wilson (2021) — vertical integration, referrals, and provider behavior\nCapps, Dranove, and Ody (2018) — foreclosure and price effects of vertical integration\n\n\n\n\n\nReferences\n\nCapps, Cory, David Dranove, and Christopher Ody. 2018. “The Effect of Hospital Acquisitions of Physician Practices on Prices and Spending.” Journal of Health Economics 59: 139–52.\n\n\nCuesta, José Ignacio, Carlos Noton, and Benjamin Vatter. 2019. “Vertical Integration Between Hospitals and Insurers.” {SSRN} {Scholarly} {Paper}. Rochester, NY. https://doi.org/10.2139/ssrn.3309218.\n\n\nKoch, Thomas G., Brett W. Wendling, and Nathan E. Wilson. 2021. “The Effects of Physician and Hospital Integration on Medicare Beneficiaries’ Health Outcomes.” The Review of Economics and Statistics 103 (4): 725–39. https://doi.org/10.1162/rest_a_00924."
  },
  {
    "objectID": "schedule/4-2.html",
    "href": "schedule/4-2.html",
    "title": "Bargaining and Hospital Pricing",
    "section": "",
    "text": "While reduced-form merger studies provide compelling evidence that consolidation raises hospital prices, they are less informative about the mechanisms through which prices are set and how alternative market structures would affect outcomes. To address these questions, a large literature models hospital prices as the outcome of bilateral bargaining between hospitals and insurers. In these models, negotiated prices depend on the relative bargaining power of each side, the value of inclusion in insurer networks, and the availability of alternative providers and plans.\nThe bargaining framework provides a structural interpretation of hospital pricing that links market structure, network design, and prices in a unified way. By explicitly modeling negotiations, these approaches allow researchers to simulate counterfactual scenarios—such as mergers that have not occurred, changes in network inclusion, or regulatory interventions—and to decompose price effects into changes in bargaining leverage versus underlying costs or demand. This makes bargaining models particularly attractive for policy analysis, despite their stronger assumptions relative to reduced-form designs.\nWe introduce this literature by focusing on empirical models that estimate negotiated prices and bargaining parameters using detailed claims and network data. These papers illustrate how bargaining power varies across hospitals and insurers, how market concentration affects negotiated outcomes, and how structural models complement reduced-form evidence from merger retrospectives. Together, they provide the analytical foundation for understanding hospital pricing in modern healthcare markets.\nPotential papers for presentation today include:\n\nGowrisankaran, Nevo, and Town (2015) — a canonical model of insurer–hospital bargaining and negotiated prices\nLewis and Pflum (2015) — evidence on bargaining power and system-level pricing effects\nHo and Lee (2019) — bargaining with threats of exclusion\n\n\n\n\n\nReferences\n\nGowrisankaran, Gautam, Aviv Nevo, and Robert Town. 2015. “Mergers When Prices Are Negotiated: Evidence from the Hospital Industry.” American Economic Review 105 (1): 172–203.\n\n\nHo, Kate, and Robin S. Lee. 2019. “Equilibrium Provider Networks: Bargaining and Exclusion in Health Care Markets.” American Economic Review 109 (2): 473–522. https://doi.org/10.1257/aer.20171288.\n\n\nLewis, Matthew, and Kevin Pflum. 2015. “Diagnosing Hospital System Bargaining Power in Managed Care Networks.” American Economic Journal: Economic Policy 7 (1): 243–74."
  },
  {
    "objectID": "schedule/4-0.html",
    "href": "schedule/4-0.html",
    "title": "Understanding Hospital Competition",
    "section": "",
    "text": "A central challenge in studying hospital pricing is that competition in healthcare operates very differently than in standard goods markets. Hospitals provide highly differentiated services, patients are largely insulated from prices at the point of care, and prices are determined through bilateral negotiations between hospitals and insurers rather than posted prices. Physician referral patterns, insurer network design, and regulatory constraints further complicate how demand responds to prices and how market power is exercised. As a result, familiar notions of competition and marginal-cost pricing must be adapted to fit the institutional realities of hospital markets.\nThe literature on hospital competition develops frameworks for understanding how these institutional features shape prices, quality, and welfare. Rather than focusing solely on patient choice, this work emphasizes bargaining between hospitals and insurers, the role of outside options created by network inclusion, and the importance of market definition in differentiated product markets. Early contributions established that hospital market structure is closely linked to prices and outcomes, while later work refined empirical strategies and theoretical models to better capture negotiated pricing and strategic interaction.\nWe introduce this literature by drawing on survey and review papers that synthesize the key mechanisms underlying hospital competition and market power. These readings clarify why consolidation has the potential to raise prices even in nonprofit settings, why standard concentration measures can be misleading, and how institutional details matter for both empirical measurement and policy analysis. This foundation provides the context for the next classes, which examine reduced-form evidence on hospital mergers and structural models of insurer–hospital bargaining.\nPotential papers for presentation today include:\n\nMartin Gaynor and Vogt (2003) — early conceptual and empirical analysis of hospital competition\nM. Gaynor, Ho, and Town (2015) — a comprehensive synthesis of theory and evidence on hospital competition and pricing\n\n\n\n\n\nReferences\n\nGaynor, Martin, and William B Vogt. 2003. “Competition Among Hospitals.” RAND Journal of Economics, 764–85.\n\n\nGaynor, M, K Ho, and R Town. 2015. “The Industrial Organization of Health Care Markets.” Journal of Economic Literature 47 (2): 235–84."
  },
  {
    "objectID": "schedule/3-2.html",
    "href": "schedule/3-2.html",
    "title": "Patient-Treatment Match Value",
    "section": "",
    "text": "An important source of physician learning arises from heterogeneity in patient responses to treatment. Even when average treatment effects are well understood, physicians often face uncertainty about how particular patients will respond, creating scope for learning about patient–treatment match quality. This form of learning is central to personalized medicine and provides a dynamic explanation for variation in treatment patterns across physicians and over time.\nThe literature on learning about match value emphasizes that physicians update beliefs not only about treatments in general, but about how treatments perform for different types of patients. Learning therefore depends on diagnostic signals, observed outcomes, and the physician’s willingness to experiment across patient populations. Early structural work in pharmaceutical markets formalizes this idea by modeling how learning about heterogeneous treatment effects shapes prescribing behavior following new drug entry (Coscelli and Shum (2004); Crawford and Shum (2005)). These models show that uncertainty about match quality can generate gradual diffusion, persistence in prescribing, and patient-level sorting across treatments.\nMore recent work brings these ideas directly into clinical settings, highlighting the role of diagnostic skill and information in guiding experimentation. Currie and MacLeod (2020) develops a model in which physicians learn about both treatment effectiveness and patient-specific match quality, showing how higher diagnostic skill leads to better targeting of therapies and improved patient outcomes. Together, this literature frames learning about match value as a key mechanism linking uncertainty, experimentation, and heterogeneity in care, distinct from both learning-by-doing and social learning.\nPotential papers for presentation today include:\n\nCoscelli and Shum (2004) — learning about heterogeneous treatment effects with patient spillovers\nCrawford and Shum (2005) — dynamic learning and matching in pharmaceutical demand\nCurrie and MacLeod (2020) — physician learning, diagnostic skill, and treatment matching\n\n\n\n\n\nReferences\n\nCoscelli, Andrea, and Matthew Shum. 2004. “An Empirical Model of Learning and Patient Spillovers in New Drug Entry.” Journal of Econometrics 122 (2): 213–46. https://doi.org/10.1016/j.jeconom.2003.09.002.\n\n\nCrawford, Gregory S, and Matthew Shum. 2005. “Uncertainty and Learning in Pharmaceutical Demand.” Econometrica 73 (4): 1137–73.\n\n\nCurrie, Janet M., and W. Bentley MacLeod. 2020. “Understanding Doctor Decision Making: The Case of Depression Treatment.” Econometrica 88 (3): 847–78. https://doi.org/10.3982/ECTA16591."
  },
  {
    "objectID": "schedule/3-0.html",
    "href": "schedule/3-0.html",
    "title": "Learning Under Uncertainty",
    "section": "",
    "text": "An important feature of medical decision-making is that physicians often operate under substantial uncertainty about treatment effectiveness, side effects, and patient-specific responses. Much of clinical knowledge is acquired outside of formal training, through experience, feedback from outcomes, and exposure to new information. As a result, treatment decisions reflect not only preferences and incentives, but also evolving beliefs shaped by incomplete and noisy signals.\nThe literature on physician learning provides a framework for understanding how doctors update beliefs over time and how uncertainty influences treatment choice. Learning models formalize the idea that physicians face trade-offs between exploiting treatments they believe to be effective and experimenting with alternatives to gain information. These dynamics can generate persistence in practice styles, delayed adoption of new technologies, and heterogeneity in care even among observationally similar physicians.\nWe introduce this literature using Ching, Erdem, and Keane (2013), which reviews the development of empirical learning models in consumer and physician behavior over the past two decades. Building on foundational work such as Erdem and Keane (1996), the paper highlights how dynamic learning models have improved our understanding of decision-making under uncertainty, while also emphasizing key empirical challenges. In particular, it discusses the difficulty of distinguishing learning from other sources of persistence, such as habit formation or switching costs, and the importance of directly measuring information, beliefs, and expectations. This framework provides a common language for the remainder of the module, where we examine specific objects of learning and their implications for treatment decisions and technology adoption.\nPotential papers for presentation today include:\n\nChing, Erdem, and Keane (2013) — a synthesis of learning models and their empirical challenges\nChan, Narasimhan, and Xie (2013) — a physician-focused model of learning about treatment effectiveness and side effects\n\n\n\n\n\nReferences\n\nChan, Tat, Chakravarthi Narasimhan, and Ying Xie. 2013. “Treatment Effectiveness and Side Effects: A Model of Physician Learning.” Management Science 59 (6): 1309–25.\n\n\nChing, Andrew T., Tülin Erdem, and Michael P. Keane. 2013. “Learning Models: An Assessment of Progress, Challenges, and New Developments.” Marketing Science 32 (6): 913–38. https://doi.org/10.1287/mksc.2013.0805.\n\n\nErdem, Tülin, and Michael P. Keane. 1996. “Decision-Making Under Uncertainty: Capturing Dynamic Brand Choice Processes in Turbulent Consumer Goods Markets.” Marketing Science 15 (1): 1–20. https://www.jstor.org/stable/184181."
  },
  {
    "objectID": "schedule/2-2.html",
    "href": "schedule/2-2.html",
    "title": "Agency and Non-financial Incentives",
    "section": "",
    "text": "While financial incentives operate through explicit changes in physician reimbursement, many important sources of physician agency arise from incentives that do not directly affect pay. In this class, we use non-financial incentives to refer to forces that shape physician behavior through professional norms, reputation, peer interactions, information, and institutional constraints, rather than through marginal payments for specific services. These incentives matter even when prices are held fixed and help explain why physicians facing similar financial environments may practice very differently. For example, physicians may internalize local standards of care, respond to peer behavior, or adjust treatment choices to avoid malpractice risk. Empirical work documents persistent physician-specific practice styles that affect both utilization and patient outcomes, even after controlling for patient characteristics and observable incentives (Currie, MacLeod, and Van Parys (2016); Molitor (2018)). These findings highlight that agency is not simply a response to prices, but also reflects learned behavior and social context.\nOther work emphasizes how informational frictions and expertise shape decision-making. Physicians often act as experts guiding patient choices, which can generate agency problems even in the absence of direct financial gain. For instance, referral decisions and diagnostic intensity may reflect physician beliefs, habits, or perceived responsibility rather than reimbursement alone. Studies exploiting variation in referral networks and physician mobility show that these non-financial channels can generate substantial differences in care delivery (Epstein and Nicholson (2009); Zeltzer (2020)).\nTogether, this literature demonstrates that non-financial incentives are central to understanding physician agency. These mechanisms complement—but are distinct from—financial incentives, and they motivate a broader view of physician behavior that incorporates norms, information, and social interactions. Potential papers for presentation today include:\n\nCurrie, MacLeod, and Van Parys (2016) — physician practice styles and patient health outcomes\nMolitor (2018) — evolution of physician behavior through migration\nZeltzer (2020) — referral networks, homophily, and physician behavior\n\n\n\n\n\nReferences\n\nCurrie, Janet, W. Bentley MacLeod, and Jessica Van Parys. 2016. “Provider Practice Style and Patient Health Outcomes: The Case of Heart Attacks.” Journal of Health Economics 47 (May): 64–80. https://doi.org/10.1016/j.jhealeco.2016.01.013.\n\n\nEpstein, Andrew J., and Sean Nicholson. 2009. “The Formation and Evolution of Physician Treatment Styles: An Application to Cesarean Sections.” Journal of Health Economics 28 (6): 1126–40. https://doi.org/10.1016/j.jhealeco.2009.08.003.\n\n\nMolitor, David. 2018. “The Evolution of Physician Practice Styles: Evidence from Cardiologist Migration.” American Economic Journal: Economic Policy 10 (1): 326–56.\n\n\nZeltzer, Dan. 2020. “Gender Homophily in Referral Networks: Consequences for the Medicare Physician Earnings Gap.” American Economic Journal: Applied Economics 12 (2): 169–97. https://doi.org/10.1257/app.20180201."
  },
  {
    "objectID": "schedule/2-0.html",
    "href": "schedule/2-0.html",
    "title": "Physician Agency",
    "section": "",
    "text": "A central empirical fact motivating the study of physician agency is the substantial variation in healthcare utilization and spending across geographic areas and providers. Early work documenting small-area variation showed that patients with similar observable characteristics receive markedly different levels and types of care depending on where they live and which physicians they see (J. Wennberg and Gittelsohn (1973); J. E. Wennberg, Fisher, and Skinner (2004)). While some of this variation reflects differences in patient health and preferences, a large literature demonstrates that supply-side factors play an important role, raising questions about how treatment decisions are made in environments characterized by asymmetric information and physician discretion.\nPhysician agency provides a unifying framework for understanding these patterns. Physicians act as agents for patients, but they typically have discretion over diagnostic and treatment choices and face incentives that may not be perfectly aligned with patient welfare. Classic conceptual work formalizes this agency problem and emphasizes how physician objectives, information, and institutional constraints shape care delivery (McGuire (2000)). More recent empirical research builds on this framework to quantify the extent to which physicians contribute to observed variation in healthcare use, often exploiting patient or physician mobility to separate demand-side factors from physician practice styles (Finkelstein, Gentzkow, and Williams (2016); Badinski et al. (2023)).\nTogether, this body of work highlights physician agency as a key mechanism linking institutional features of healthcare markets to real differences in treatment intensity and patient outcomes. It also provides a foundation for subsequent discussions of how financial and non-financial incentives, as well as organizational structure, shape physician behavior.\nPotential papers for presentation today include:\n\nFinkelstein, Gentzkow, and Williams (2016) — separating demand- and supply-side sources of healthcare variation\nBadinski et al. (2023) — the role of physicians in explaining geographic variation in utilization\n\n\n\n\n\nReferences\n\nBadinski, Ivan, Amy Finkelstein, Matthew Gentzkow, and Peter Hull. 2023. “Geographic Variation in Healthcare Utilization: The Role of Physicians.” Working {Paper}. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w31749.\n\n\nFinkelstein, Amy, Matthew Gentzkow, and Heidi Williams. 2016. “Sources of Geographic Variation in Health Care: Evidence From Patient Migration.” The Quarterly Journal of Economics 131 (4): 1681–1726. https://ideas.repec.org//a/oup/qjecon/v131y2016i4p1681-1726..html.\n\n\nMcGuire, Thomas G. 2000. “Physician Agency.” Handbook of Health Economics 1: 461–536.\n\n\nWennberg, John E., Elliott S. Fisher, and Jonathan S. Skinner. 2004. “Geography And The Debate Over Medicare Reform.” Health Affairs, W96–114. https://www.proquest.com/docview/204500754/abstract/1F1C6E2B7FD14576PQ/1.\n\n\nWennberg, John, and Alan Gittelsohn. 1973. “Small Area Variations in Health Care Delivery: A Population-Based Health Information System Can Guide Planning and Regulatory Decision-Making.” Science 182 (4117): 1102–8."
  },
  {
    "objectID": "schedule/1-3.html",
    "href": "schedule/1-3.html",
    "title": "Competition and Market Structure",
    "section": "",
    "text": "A large literature studies how competition among insurers shapes premiums, plan offerings, and consumer outcomes in health insurance markets. While standard economic intuition suggests that increased competition should lower prices, identifying these effects empirically is challenging in health insurance due to regulation, benefit standardization, and interactions with provider prices. As a result, much of the empirical literature focuses on isolating plausibly exogenous variation in market structure and examining how insurer concentration and entry affect equilibrium premiums and plan characteristics.\nOne strand of this literature examines the relationship between insurer concentration and premiums. Empirical evidence shows that more concentrated insurance markets tend to have higher premiums, though the magnitude of these effects depends on market definition and institutional context. These studies emphasize that insurer market power is an important determinant of prices, even in settings with substantial regulation (Dafny, Duggan, and Ramanarayanan (2012); Ho and Lee (2017)).\nMore recent work highlights that competition can also operate along non-price margins, including plan design and networks, suggesting that the effects of competition may extend beyond premiums alone. Together, this literature provides a foundation for understanding when and how insurer competition delivers consumer benefits in health insurance markets.\nPotential papers for presentation today include\n\nDafny, Duggan, and Ramanarayanan (2012) — insurer concentration and health insurance premiums\nHo and Lee (2017) — competition, plan design, and non-price margins\n\n\n\n\n\nReferences\n\nDafny, Leemore, Mark Duggan, and Subramaniam Ramanarayanan. 2012. “Paying a Premium on Your Premium? Consolidation in the US Health Insurance Industry.” American Economic Review 102 (2): 1161–85.\n\n\nHo, Kate, and Robin S Lee. 2017. “Insurer Competition in Health Care Markets.” Econometrica 85 (2): 379–417."
  },
  {
    "objectID": "schedule/1-1.html",
    "href": "schedule/1-1.html",
    "title": "Welfare Effects of Health Insurance",
    "section": "",
    "text": "Today, we move on from why people buy health insurance to the effects of health insurance, with particular attention to financial well-being, health, and broader welfare implications. One strand of this literature focuses on the financial effects of health insurance. By reducing exposure to medical expenditures, insurance can relax household budget constraints and lower the likelihood of financial distress. Empirical evidence exploiting Medicaid expansions shows that gaining public insurance reduces adverse financial outcomes such as medical debt and bankruptcy, highlighting an important channel through which insurance affects economic well-being beyond healthcare consumption (Gross and Notowidigdo (2011); Hu et al. (2018)).\nA second strand examines the health effects of insurance coverage. While it is not theoretically obvious that insurance must improve health, recent work using large administrative datasets and quasi-experimental designs provides compelling evidence that expanded coverage improves health outcomes and reduces mortality. These studies reflect a shift in the literature toward settings and data that are sufficiently powered to detect health effects over longer horizons (Miller, Johnson, and Wherry (2021); Goldin, Lurie, and McCubbin (2021)).\nFinally, a broader literature considers the overall welfare effects of health insurance, emphasizing that insurance policies generate indirect effects that extend beyond the insured population. The introduction and expansion of public insurance programs can affect provider behavior, healthcare spending, and the allocation of resources across the economy. Classic and recent work shows that large-scale insurance expansions such as Medicare and Medicaid have substantial general equilibrium and fiscal implications, underscoring the importance of evaluating insurance policies from a system-wide perspective rather than focusing solely on individual beneficiaries (Finkelstein (2007); Finkelstein, Hendren, and Luttmer (2019)).\nPotential papers for presentation today include\n\nHu et al. (2018) — financial protection and household debt under Medicaid\nMiller, Johnson, and Wherry (2021) — health and mortality effects of insurance coverage\nFinkelstein, Hendren, and Luttmer (2019) — welfare valuation of Medicaid incorporating transfers and external effects\n\n\n\n\n\nReferences\n\nFinkelstein, Amy. 2007. “The Aggregate Effects of Health Insurance: Evidence from the Introduction of Medicare.” The Quarterly Journal of Economics 122 (1): 1–37. https://doi.org/10.1162/qjec.122.1.1.\n\n\nFinkelstein, Amy, Nathaniel Hendren, and Erzo F. P. Luttmer. 2019. “The Value of Medicaid: Interpreting Results from the Oregon Health Insurance Experiment.” Journal of Political Economy 127 (6): 2836–74. https://doi.org/10.1086/702238.\n\n\nGoldin, Jacob, Ithai Z Lurie, and Janet McCubbin. 2021. “Health Insurance and Mortality: Experimental Evidence from Taxpayer Outreach.” The Quarterly Journal of Economics 136 (1): 1–49. https://doi.org/10.1093/qje/qjaa029.\n\n\nGross, Tal, and Matthew J. Notowidigdo. 2011. “Health Insurance and the Consumer Bankruptcy Decision: Evidence from Expansions of Medicaid.” Journal of Public Economics 95 (7): 767–78. https://doi.org/10.1016/j.jpubeco.2011.01.012.\n\n\nHu, Luojia, Robert Kaestner, Bhashkar Mazumder, Sarah Miller, and Ashley Wong. 2018. “The Effect of the Affordable Care Act Medicaid Expansions on Financial Wellbeing.” Journal of Public Economics 163 (July): 99–112. https://doi.org/10.1016/j.jpubeco.2018.04.009.\n\n\nMiller, Sarah, Norman Johnson, and Laura R Wherry. 2021. “Medicaid and Mortality: New Evidence From Linked Survey and Administrative Data.” The Quarterly Journal of Economics 136 (3): 1783–1829. https://doi.org/10.1093/qje/qjab004."
  },
  {
    "objectID": "schedule/0-0.html",
    "href": "schedule/0-0.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the course! Today we’ll spend some time talking about the structure of the class, the assignments, grading, etc. We’ll also navigate the website for a bit just to make sure we can get around and understand where different information is. But the main goal for today is just to get to know each other and our collective research interests."
  },
  {
    "objectID": "assignments/research-proposals.html",
    "href": "assignments/research-proposals.html",
    "title": "Research Proposals and Plan",
    "section": "",
    "text": "The research proposal is a starting point for a research project. It’s a basic structure that can help you think through specific aspects of a new project and begin to assess its viability and its potential contribution. Each proposal is worth 10 points toward your final grade and should be 2 pages (not including a bibliography), double-spaced, with 12-point font and 1-inch margins. You’ll submit three proposals thoughout the semester (due dates of February 5, February 26, and March 26).\nEach research proposal should consist of the following sections, with each section worth 2 points toward the proposal grade. An additional 2 points apply to meeting the formatting requirements.\n\n\nThis should be formulated as an input-output question, e.g., I’m interested in studying the effects of T on Y, or I’m interested in understanding the association between X and Y. While there are many types of research questions, in general your question should satisfy three key criteria (adapted from Jesse Shapiro’s “Four Steps to an Applied Micro Paper”)\n\nThe question has an answer. We may not know what the answer is, but we can agree that an answer exists. “What is the effect on quantity sold of imposing a price ceiling on ketchup?” has an answer. “What is the nature of firm and consumer behavior in the ketchup industry?” does not.\nThe answer is not obvious. We cannot readily determine the answer to the question based on logic, common sense, or existing evidence. “Would making ketchup available only on Tuesdays improve consumer welfare?” has an answer, but we can probably guess what it is.\nThe answer is actionable. If we knew the answer, someone (say, a policymaker, firm, or NGO) might care to do things differently. “What is the effect on consumer welfare of imposing uniform wholesale pricing on the ketchup market?” seems actionable, because in fact ketchup is subject to uniform wholesale pricing restrictions in the US, and a policymaker might be interested to know whether such restrictions are a good idea. “What is the effect on consumer welfare of a technology that instantly transports ketchup to your fries whenever you need it?” seems more hypothetical.\n\n\n\n\nWhy is your question important and relevant? This should be aimed at a general audience, not someone specific to your field and perhaps not even to an economist.\n\n\n\nYour research question should be specific enough so that you are able to write down a model, an equation, or a series of equations that would facilitate answering this question. The model is not simply a mathematical representation of your research question. Rather, it should help inform the empirical methods and variation you will be using.\n\n\n\nYou must provide a detailed description of the data that you plan to use to estimate the model and answer the research question. This must include a discussion of where the variation in the data is coming from, why is it credible, and how it maps onto the research question. You don’t need to have the data yet, but you need to know it exists and how to access it. The data must also be accessible in a reasonable time period. Course instructors are expected to verify the existence and feasibility of obtaining the data.",
    "crumbs": [
      "Assignments",
      "Research Proposals and Plan"
    ]
  },
  {
    "objectID": "assignments/research-proposals.html#research-proposal",
    "href": "assignments/research-proposals.html#research-proposal",
    "title": "Research Proposals and Plan",
    "section": "",
    "text": "The research proposal is a starting point for a research project. It’s a basic structure that can help you think through specific aspects of a new project and begin to assess its viability and its potential contribution. Each proposal is worth 10 points toward your final grade and should be 2 pages (not including a bibliography), double-spaced, with 12-point font and 1-inch margins. You’ll submit three proposals thoughout the semester (due dates of February 5, February 26, and March 26).\nEach research proposal should consist of the following sections, with each section worth 2 points toward the proposal grade. An additional 2 points apply to meeting the formatting requirements.\n\n\nThis should be formulated as an input-output question, e.g., I’m interested in studying the effects of T on Y, or I’m interested in understanding the association between X and Y. While there are many types of research questions, in general your question should satisfy three key criteria (adapted from Jesse Shapiro’s “Four Steps to an Applied Micro Paper”)\n\nThe question has an answer. We may not know what the answer is, but we can agree that an answer exists. “What is the effect on quantity sold of imposing a price ceiling on ketchup?” has an answer. “What is the nature of firm and consumer behavior in the ketchup industry?” does not.\nThe answer is not obvious. We cannot readily determine the answer to the question based on logic, common sense, or existing evidence. “Would making ketchup available only on Tuesdays improve consumer welfare?” has an answer, but we can probably guess what it is.\nThe answer is actionable. If we knew the answer, someone (say, a policymaker, firm, or NGO) might care to do things differently. “What is the effect on consumer welfare of imposing uniform wholesale pricing on the ketchup market?” seems actionable, because in fact ketchup is subject to uniform wholesale pricing restrictions in the US, and a policymaker might be interested to know whether such restrictions are a good idea. “What is the effect on consumer welfare of a technology that instantly transports ketchup to your fries whenever you need it?” seems more hypothetical.\n\n\n\n\nWhy is your question important and relevant? This should be aimed at a general audience, not someone specific to your field and perhaps not even to an economist.\n\n\n\nYour research question should be specific enough so that you are able to write down a model, an equation, or a series of equations that would facilitate answering this question. The model is not simply a mathematical representation of your research question. Rather, it should help inform the empirical methods and variation you will be using.\n\n\n\nYou must provide a detailed description of the data that you plan to use to estimate the model and answer the research question. This must include a discussion of where the variation in the data is coming from, why is it credible, and how it maps onto the research question. You don’t need to have the data yet, but you need to know it exists and how to access it. The data must also be accessible in a reasonable time period. Course instructors are expected to verify the existence and feasibility of obtaining the data.",
    "crumbs": [
      "Assignments",
      "Research Proposals and Plan"
    ]
  },
  {
    "objectID": "assignments/research-proposals.html#research-plan",
    "href": "assignments/research-proposals.html#research-plan",
    "title": "Research Proposals and Plan",
    "section": "Research Plan",
    "text": "Research Plan\nYour research plan consists of a three-page report addressing the following five points:\n\nWhat is your research question? (from research proposal)\nWhat is the hook? (from research proposal)\nWhat is your economic model? (from research proposal)\nWhat is the data and source of variation used for identification? (from research proposal)\nWhat is the value-added? Given points (1) to (4), what do we know in the literature about this problem? What are the relevant papers when it comes to the topic, methods, and data? How exactly do you contribute to the literature, above and beyond what was already done. Note that doing the same thing for a different population or using different methods or different data is not exciting unless the hook is related to the specific population, data, method, etc. The research plan is therefore an extension of the research proposal that includes the equivalent of a small-scale literature review.\n\nThe research plan is worth 10 points toward your final grade (2 points per section of the report). It is due via Canvas on April 21. You will also present your final research plan to the class on April 21 and April 23.",
    "crumbs": [
      "Assignments",
      "Research Proposals and Plan"
    ]
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Below are basic descriptions of each category of assignments for this semester. More details are available on the individual pages for each assignment.",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#research-proposals-and-plan",
    "href": "assignments/index.html#research-proposals-and-plan",
    "title": "Assignments",
    "section": "Research Proposals and Plan",
    "text": "Research Proposals and Plan\nYou’ll submit three proposals thoughout the semester. Each proposal should consist of four parts, explained in more detail here, and is worth 10 points toward your final grade. The written proposal should be 2 pages, double-spaced, 12-point font, and 1-inch margins.\nThink of the research proposal as a more informed version of brainstorming. This should be something for which you’ve given some serious thought but where you haven’t yet started any analysis. You at least have some idea of the data you’d like to use, where to find it, and how to use it (i.e., you have some identification strategy and estimation method in mind). The purpose of the proposal is to develop an idea sufficiently far before receiving feedback.\nYour final research plan consists of five parts is an extension of one research proposal where you consider the value-added of your work relative to the existing literature. Essentially, the research plan is your research proposal plus a literature review/contribution. You should take your best research proposal, based on feedback from me (and possibly your peers and classmates), and develop a slightly longer three-page report (double-spaced, 12-point font, 1-inch margins).\nThe research plan is worth 10 points toward your final grade (2 points per section of the report). It is due via Canvas by April 21. You will also need to develop a short 15 minute presentation for your research plan. We’ll spend the final two class days discussing your research plans and how best to turn them into papers!",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#paper-presentations",
    "href": "assignments/index.html#paper-presentations",
    "title": "Assignments",
    "section": "Paper Presentations",
    "text": "Paper Presentations\nYou will present four published papers throughout the course of the semester. The list of potential papers for any given day is on the class schedule, with additional details here. Please note your selected papers and class dates on the presentations tab of our shared Google Sheet (link availabel on Canvas) no later than January 16. Each presentation is worth 5 points toward your final grade.\nIn addition to these 4 graded presentations, I expect everyone to present and discuss recent job market papers toward the end of the semester (Module 6). While ungraded, these JMP presentations will help everyone better understand the quality and rigor of recent top job market papers in our field. I will provide a list of candidate papers later in the semester.",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#empirical-exercises",
    "href": "assignments/index.html#empirical-exercises",
    "title": "Assignments",
    "section": "Empirical Exercises",
    "text": "Empirical Exercises\nThere are four prepared empirical exercises. Three of the excercises focus on central causal inference strategies (difference-in-differences, instrumental variables, or regression discontinuity), and the fourth exercise considers the analysis of competition in healthcare, including demand estimation. You must select one exercise to complete throughout the semester no later than January 29, and note your selection on the exercises tab of our shared Google Sheet. No more than two students per exercise.\nEach exercise requires a good amount of your time outside of class to get the data in working order and implement the relevant identification strategy and econometric estimator. Raw data for each exercise will be provided on our class OneDrive folder, the link to which is on Canvas, or via the paper’s online replication package. For more details on each exercise and on the requirements for submission, see here. Your final submission is due on the last day of class, April 23.",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/index.html#excellence-blueprint",
    "href": "assignments/index.html#excellence-blueprint",
    "title": "Assignments",
    "section": "“Excellence” Blueprint",
    "text": "“Excellence” Blueprint\nThe objective of this assignment is to move beyond critiquing flaws and instead develop an appreciation for very high quality academic writing and research execution. You will analyze a recently published, high-impact health economics paper, dissecting not only its substantive contribution and rigor but also the writing, narrative, and presentation that secured its placement in a top-tier journal. Your final blueprint is due on March 17, and details of this assignment are available here.",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/exercise3.html",
    "href": "assignments/exercise3.html",
    "title": "Exercise 3: Regression Discontinuity",
    "section": "",
    "text": "Overview\nIn this assignment, we’re going to work through some applied issues related to regression discontinuity designs. We’ll cover the basics of strict and fuzzy RD, and we’ll work through standard specification tests. We’ll also introduce some more technical aspects of bin and bandwidth selection.\n\n\nResources and data\nThe data for this assignment comes from the AEJ: Policy website, where Keith Ericson’s complete dataset is available. The data are available here. I’ve also uploaded the data to our class OneDrive folder under the directory “Ericson 2014.”\n\n\nQuestions\nIn your GitHub repository, please be sure to clearly address/answer the following questions.\n\nRecreate the table of descriptive statistics (Table 1) from Ericson (2014).\nRecreate Figure 3 from Ericson (2014).\nCalonico, Cattaneo, and Titiunik (2015) discuss the appropriate partition size for binned scatterplots such as that in Figure 3 of Ericson (2014). More formally, denote by \\(\\mathcal{P}_{-,n} = \\{ P_{-,j} : j=1, 2, ... J_{-, n} \\}\\) and \\(\\mathcal{P}_{+,n} = \\{ P_{+,j} : j=1, 2, ... J_{+, n} \\}\\) the partitions of the support of the running variable \\(x_{i}\\) on the left and right (respectively) of the cutoff, \\(\\bar{x}\\). \\(P_{-, j}\\) and \\(P_{+, n}\\) denote the actual supports for each \\(j\\) partition of size \\(J_{-,n}\\) and \\(J_{+,n}\\), such that \\([x_{l}, \\bar{x}) = \\bigcup_{j=1}^{J_{-,n}} P_{-, j}\\) and \\((\\bar{x}, x_{u}] = \\bigcup_{j=1}^{J_{+,n}} P_{+, j}\\). Individual bins are denoted by \\(p_{-,j}\\) and \\(p_{+,j}\\). With this notation in hand, we can write the partitions \\(J_{-,n}\\) and \\(J_{+,n}\\) with equally-spaced bins as \\[p_{-,j}=x_{l} + j \\times \\frac{\\bar{x} - x_{l}}{J_{-,n}},\\] and \\[p_{+,j} = \\bar{x} + j \\times \\frac{x_{u} - \\bar{x}}{J_{+,n}}.\\] Recreate Figure 3 from Ericson (2014) using \\(J_{-,n}=J_{+,n}=10\\) and \\(J_{-,n}=J_{+,n}=30\\). Discuss your results and compare them to your figure in Part 2.\nWith the notation above, Calonico, Cattaneo, and Titiunik (2015) derive the optimal number of partitions for an evenly-spaced (ES) RD plot. They show that \\[J_{ES,-,n} = \\left\\lceil \\frac{V_{-}}{\\mathcal{V}_{ES,-}} \\frac{n}{\\text{log}(n)^{2}} \\right\\rceil\\] and \\[J_{ES,+,n} = \\left\\lceil \\frac{V_{+}}{\\mathcal{V}_{ES,+}} \\frac{n}{\\text{log}(n)^{2}} \\right\\rceil,\\] where \\(V_{-}\\) and \\(V_{+}\\) denote the sample variance of the subsamples to the left and right of the cutoff and \\(\\mathcal{V}_{ES,.}\\) is an integrated variance term derived in the paper. Use the rdrobust package in R (or Stata or Python) to find the optimal number of bins with an evenly-spaced binning strategy. Report this bin count and recreate your binned scatterplots from parts 2 and 3 based on the optimal bin number.\nOne key underlying assumption for RD design is that agents cannot precisely manipulate the running variable. While “precisely” is not very scientific, we can at least test for whether there appears to be a discrete jump in the running variable around the threshold. Evidence of such a jump may suggest that manipulation is present. Provide the results from the manipulation tests described in Cattaneo, Jansson, and Ma (2018). This test can be implemented with the rddensity package in R, Stata, or Python.\nRecreate Panels A and B of Table 3 in Ericson (2014) using the same bandwidth of $4.00 but without any covariates.\nCalonico, Cattaneo, and Farrell (2020) show that pre-existing optimal bandwidth calculations (such as those used in Ericson (2014)) are invalid for appropriate inference. They propose an alternative method to derive minimal coverage error (CE)-optimal bandwidths. Re-estimate your RD results using the CE-optimal bandwidth (rdrobust will do this for you) and compare the bandwidth and RD estimates to that in Table 3 of Ericson (2014).\nNow let’s extend the analysis in Section V of Ericson (2014) using IV. Use the presence of Part D low-income subsidy as an IV for market share to examine the effect of market share in 2006 on future premium changes.\nDiscuss your findings and compare results from different binwidths and bandwidths. Compare your results in part 8 to the invest-then-harvest estimates from Table 4 in Ericson (2014).\nReflect on this assignment. What did you find most challenging? What did you find most surprising?\n\n\n\n\n\n\n\n\n\nReferences\n\nCalonico, Sebastian, Matias D Cattaneo, and Max H Farrell. 2020. “Optimal Bandwidth Choice for Robust Bias-Corrected Inference in Regression Discontinuity Designs.” The Econometrics Journal 23 (2): 192–210. https://doi.org/10.1093/ectj/utz022.\n\n\nCalonico, Sebastian, Matias D. Cattaneo, and Rocío Titiunik. 2015. “Optimal Data-Driven Regression Discontinuity Plots.” Journal of the American Statistical Association 110 (512): 1753–69. https://doi.org/10.1080/01621459.2015.1017578.\n\n\nCattaneo, Matias D, Michael Jansson, and Xinwei Ma. 2018. “Manipulation Testing Based on Density Discontinuity.” The Stata Journal 18 (1): 234–61.\n\n\nEricson, Keith M. 2014. “Consumer Inertia and Firm Pricing in the Medicare Part D Prescription Drug Insurance Exchange.” American Economic Journal: Economic Policy 6 (1): 38–64."
  },
  {
    "objectID": "assignments/excellence-blueprint.html",
    "href": "assignments/excellence-blueprint.html",
    "title": "Excellence Blueprint",
    "section": "",
    "text": "Your analysis must answer the core question: What specific choices, in both research design and articulation, warrant this paper’s strong publication and reputation? In answering this question, please divide your report into two sections (10 points for each section):\n\nThe Science: Address the paper’s substance and fit within the literature. This section should detail what the paper contributes and how its methods stand up to scrutiny.\n\nNovelty and Contribution: Clearly articulate the paper’s central research question. What specific gap in the literature does it fill? What is the single most important new insight this work provides to the field of health economics?\nMethodological Excellence: Analyze the core empirical strategy. What choices (e.g., data selection, identification strategy, or estimator) demonstrate methodological rigor? Why is the chosen method appropriate in this case?\nEngagement with Literature: Assess how the authors position their work. Identify two key papers the authors successfully build upon or challenge, and explain how the introduction and discussion sections integrate the new findings into the existing scholarly conversation.\n\nThe Art: Analyze the paper’s execution and its persuasive power. This section should detail how the authors guide the reader and present their findings clearly.\n\nIntroduction and Hook: Dissect the first two pages. How does the paper successfully motivate the question and establish its importance to both academics and policymakers? Pinpoint two specific sentences or paragraphs that are highly effective in hooking the reader and explain why they work.\nClarity and Structure: Evaluate the overall narrative flow. How do the authors simplify and present complex econometric analyses? Identify specific structural elements (e.g., headings, figures, tables) that enhance clarity and guide the reader through the evidence without sacrificing technical precision.\nThe Policy Takeaway: Analyze the conclusion and/or discussion sections. How do the authors move from specific empirical findings to a broader, impactful policy implication? Explain how the discussion ensures the paper’s findings are memorable and actionable.\n\n\nYour final product should be no longer than 4 pages (double-spaced, 11 point font, 1 inch margins), not including a bibliography. Please submit your final report in Canvas by March 17.",
    "crumbs": [
      "Assignments",
      "Excellence Blueprint"
    ]
  },
  {
    "objectID": "assignments/empirical-exercises.html",
    "href": "assignments/empirical-exercises.html",
    "title": "Empirical Exercises",
    "section": "",
    "text": "You must choose one of four empirical exercises to complete this semester. The exercises are listed and described in more detail below (or on our class website):\n\nExercise 1: Difference-in-differences\nExercise 2: Instrumental variables\nExercise 3: Regression Discontinuity\nExercise 4: Hospital markets + demand estimation\n\nPlease note which exercise you plan to complete on our shared Google Sheet by January 29 and submit your final assignment as a GitHub repository link by April 23. The exercise is worth 20 points toward your final grade, 2 points for each of 10 questions.\nIn your repository, please include a final PDF file, along with all of your supporting documentation, including your code files (Stata, R, Python, SAS, etc.), all figures/tables, and some instructions (e.g., as part of your ReadMe file) that introduce the reader to your data and the sequence in which your code should be run. Practice writing good code and showing me only what I would need to recreate your results. Please also be sure to organize your folders in a useful way (e.g., clearly organized subdirectories such as “data”, “data-code”, “analysis”, “results”, etc.). It’s good to start developing some organization practices that work best for you as early as possible. It’s extremely easy to forget what you were doing on a project once you have several things going at once, especially when you wait for 6-8 months after submitting a paper for publication. The last thing you want is to not be able to replicate your own work!",
    "crumbs": [
      "Assignments",
      "Empirical Exercises"
    ]
  },
  {
    "objectID": "assignments/exercise1.html",
    "href": "assignments/exercise1.html",
    "title": "Exercise 1: Difference-in-Differences",
    "section": "",
    "text": "Overview\nIn this exercise, we’re going to focus on causal inference in panel data, specifically on difference-in-differences and event studies. These are the workhorses of modern applied empirical microeconomics (although not as much of a panacea as once perceived). We’ll employ these research designs in the context of hospital “uncompensated care.” The simple question we want to consider is, “Does insurance expansion reduce hospital uncompensated care?”\n\n\nResources and data\nThe data for this assignment comes from three sources. These data will be made available on our class OneDrive folder. Note that you may want to use (or create) a crosswalk file between state names and abbreviations, as different datasets often have different ways to capture “state”. Stata has a user-written program, statastates, that should do what you need. R has a similar object, stcrosswalk, as part of the crosswalkr package.\n\nHospital Cost Report Information System: Hospitals submit cost reports to CMS with a wealth of (very messy) data on the hospital, including information on the extent of uncompensated care provided by the hospital in that fiscal year. You can get started with these data using my GitHub repository, HCRIS.\nProvider of Services files: These raw data (through 2019) are available from NBER POS. Data for more recent years is available directly from the CMS POS Data. The data provide information on characteristics of healthcare providers, including information on hospital ownership type. We’ll use these files to collect some basic hospital-level covariates. You can access my GitHub repository for these data here, as well as a similar repo using Stata from Adam Sacarny, available here.\nMedicaid Expansion: Data on which states expanded Medicaid under the ACA, and when they expanded, is available from the Kaiser Family Foundation website. For more info, see my GitHub repository, Insurance Access. The relevant dataset is also available on our class OneDrive folder, “KFF_medicaid_expansion_2019.txt”.\n\n\n\nQuestions\nFocus on the years from 2003 through 2019, which are years for which data on uncompensated care are available. In your GitHub repository, please be sure to clearly address/answer the following questions.\n\nProvide and discuss a table of simple summary statistics showing the mean, standard deviation, min, and max of hospital total revenues and uncompensated care over time.\nCreate a figure showing the mean hospital uncompensated care from 2003 to 2019. Show this trend separately by hospital ownership type (private not for profit and private for profit).\nUsing a simple DD identification strategy, estimate the effect of Medicaid expansion on hospital uncompensated care using a traditional two-way fixed effects (TWFE) estimation: \\[\\begin{equation}\ny_{it} = \\alpha_{i} + \\gamma_{t} + \\delta D_{it} + \\varepsilon_{it},\n(\\#eq:dd)\n\\end{equation}\\] where \\(D_{it}=1(E_{i}\\leq t)\\) in Equation @ref(eq:dd) is an indicator set to 1 when a hospital is in a state that expanded as of year \\(t\\) or earlier, \\(\\gamma_{t}\\) denotes time fixed effects, \\(\\alpha_{i}\\) denotes hospital fixed effects, and \\(y_{it}\\) denotes the hospital’s amount of uncompensated care in year \\(t\\). Present four estimates from this estimation in a table: one based on the full sample (regardless of treatment timing); one when limiting to the 2014 treatment group (with never treated as the control group); one when limiting to the 2015 treatment group (with never treated as the control group); and one when limiting to the 2016 treatment group (with never treated as the control group). Briefly explain any differences.\nEstimate an “event study” version of the specification in part 3: \\[\\begin{equation}\ny_{it} = \\alpha_{i} + \\gamma_{t} +\\sum_{\\tau &lt; -1} D_{it}^{\\tau} \\delta_{\\tau} + \\sum_{\\tau&gt;=0} D_{it}^{\\tau} \\delta_{\\tau} + \\varepsilon_{it},\n(\\#eq:event)\n\\end{equation}\\] where \\(D_{it}^{\\tau} = 1(t-E_{i}=\\tau)\\) in Equation @ref(eq:event) is essentially an interaction between the treatment dummy and a relative time dummy. In this notation and context, \\(\\tau\\) denotes years relative to Medicaid expansion, so that \\(\\tau=-1\\) denotes the year before a state expanded Medicaid, \\(\\tau=0\\) denotes the year of expansion, etc. Estimate with two different samples: one based on the full sample and one based only on those that expanded in 2014 (with never treated as the control group).\nSun and Abraham (SA) show that the \\(\\delta_{\\tau}\\) coefficients in Equation @ref(eq:event) can be written as a non-convex average of all other group-time specific average treatment effects. They propose an interaction weighted specification: \\[\\begin{equation}\ny_{it} = \\alpha_{i} + \\gamma_{t} +\\sum_{e} \\sum_{\\tau \\neq -1} \\left(D_{it}^{\\tau} \\times 1(E_{i}=e)\\right) \\delta_{e, \\tau} + \\varepsilon_{it}.\n(\\#eq:iwevent)\n\\end{equation}\\] Re-estimate your event study using the SA specification in Equation @ref(eq:iwevent). Show your results for \\(\\hat{\\delta}_{e, \\tau}\\) in a Table, focusing on states with \\(E_{i}=2014\\), \\(E_{i}=2015\\), and \\(E_{i}=2016\\).\nPresent an event study graph based on the results in part 5. Hint: you can do this automatically in R with the fixest package (using the sunab syntax for interactions), or with eventstudyinteract in Stata. These packages help to avoid mistakes compared to doing the tables/figures manually and also help to get the standard errors correct.\nCallaway and Sant’Anna (CS) offer a non-parametric solution that effectively calculates a set of group-time specific differences, \\(ATT(g,t)= E[y_{it}(g) - y_{it}(\\infty) | G_{i}=g]\\), where \\(g\\) reflects treatment timing and \\(t\\) denotes time. They show that under the standard DD assumptions of parallel trends and no anticipation, \\(ATT(g,t) = E[y_{it} - y_{i, g-1} | G_{i}=g] - E[y_{it} - y_{i,g-1} | G_{i} = \\infty]\\), so that \\(\\hat{ATT}(g,t)\\) is directly estimable from sample analogs. CS also propose aggregations of \\(\\hat{ATT}(g,t)\\) to form an overall ATT or a time-specific ATT (e.g., ATTs for \\(\\tau\\) periods before/after treatment). With this framework in mind, provide an alternative event study using the CS estimator. Hint: check out the did package in R or the csdid package in Stata.\nRambachan and Roth (RR) show that traditional tests of parallel pre-trends may be underpowered, and they provide an alternative estimator that essentially bounds the treatment effects by the size of an assumed violation in parallel trends. One such bound RR propose is to limit the post-treatment violation of parallel trends to be no worse than some multiple of the pre-treatment violation of parallel trends. Assuming linear trends, such a relative violation is reflected by \\[\\Delta(\\bar{M}) = \\left\\{ \\delta : \\forall t \\geq 0, \\lvert (\\delta_{t+1} - \\delta_{t}) - (\\delta_{t} - \\delta_{t-1}) \\rvert \\leq \\bar{M} \\times \\max_{s&lt;0} \\lvert (\\delta_{s+1} - \\delta_{s}) - (\\delta_{s} - \\delta_{s-1}) \\rvert \\right\\}.\\] The authors also propose a similar approach with what they call “smoothness restrictions,” in which violations in trends changes no more than \\(M\\) between periods. The only difference is that one restriction is imposed relative to observed trends, and one restriction is imposed using specific values. Using the HonestDiD package in R or Stata, present a sensitivity plot of your CS ATT estimates using smoothness restrictions, with assumed violations of size \\(M \\in \\left\\{ 500, 1000, 1500, 2000 \\right\\}\\). Check out the GitHub repo here for some help in combining the HonestDiD package with CS estimates. Note that you’ll need to edit the function in that repo in order to use pre-specified smoothness restrictions. You can do that by simply adding Mvec=Mvec in the createSensitivityResults function for type=smoothness.\nDiscuss your findings and compare estimates from different estimators (e.g., are your results sensitive to different specifications or estimators? Are your results sensitive to violation of parallel trends assumptions?).\nReflect on this assignment. What did you find most challenging? What did you find most surprising?"
  },
  {
    "objectID": "assignments/exercise4.html",
    "href": "assignments/exercise4.html",
    "title": "Exercise 4: Hospital Markets and Demand Estimation",
    "section": "",
    "text": "Overview\nIn this exercise, we’re going to work through some applied issues related to measuring hospital markets and estimating demand with market-level data.\n\n\nResources and data\nThe data for this assignment comes from two sources:\n\nHospital Cost Report Information System: We used these data for assignment 1. Info and code for these data are available at my GitHub page here. The raw data are available in our class OneDrive folder. Use these data for information on total discharges and to construct a measure of hospital prices.\nHospital Market GitHub Repo: John Graves has an excellent GitHub repository detailing the issues in measuring hospital markets. You can access that repo here. To replicate some of his work using the community detection algorithm, you’ll need to use the Hospital Service Area Files. The link takes you to the CMS website where you can download the data directly. For a condensed version of the code focusing more on the market formation rather than mapping, see my GitHub repo here.\n\nBut given the time crunch of the semester and the complexity of some of the raw data work for this assignment, I’m going to try to give you all a break here by skipping some steps for you. In our OneDrive folder, you’ll find the ‘hospital_markets.txt’ data, which is the data output from the hospital market github repo. I’ve also uploaded a couple of other geographic data files to give you info on HRR, zip, and fips.\n\n\nQuestions\nIn your GitHub repository, please be sure to clearly address/answer the following questions. In all cases, limit your data to the years 2000 through 2017.\n\nCalculate hospital market shares when defining the “market” by zip code and using hospital discharges as your measure of quantity. Plot the distribution of market shares over time (e.g., with a box plot or a violin plot).\nStill using zip codes as your definition of the market, provide descriptive evidence on the relationship between market concentration and price by estimating a linear regression of the following: \\[p_{ht} = \\beta HHI_{m(h)t} + \\lambda x_{ht} + \\theta z_{m(h)t} + \\gamma_{h} + \\gamma_{t} + \\varepsilon_{ht},\\] where \\(p_{ht}\\) denotes the estimated price for hospital \\(h\\) at time \\(t\\), \\(HHI_{m(h)t}\\) denotes market concentration, \\(x_{ht}\\) denotes time-varying hospital characteristics, \\(z_{m(h)t}\\) denotes time-varying market characteristics, and \\(\\gamma_{h}\\) and \\(\\gamma_{t}\\) denote hospital and time fixed effects.\nCalculate hospital market shares when defining the “market” as the HRR, again using hospital discharges as your measure of quantity. Plot the distribution of market shares over time. How do these results differ from part 1?\nStill using HRR as your definition of the market, provide descriptive evidence on the relationship between market concentration and price by estimating the same linear regression from part (2), but now with HRR as your definition of the market.\nCalculate hospital market shares when defining the market based on the community detection algorithms described here. Continue to use hospital discharges as your measure of quantity, and again provide a plot of market shares over time. How do these results differ from those in parts 1 and 3?\nUsing your community detection measure of markets, provide descriptive evidence on the relationship between market concentration and price by estimating the same linear regression from part (2), but with your new market definition.\nEstimate a logit discrete choice model using market share data based on the different definition of markets in parts 1, 3, and 5. Be sure to include year and hospital fixed effects in your estimation. What is your price elasticity estimate in each case, and how does it differ based on your measure of the market? hint: don’t worry about instrumenting for anything right now.\nDiscuss your findings and compare results from different market definitions. Also compare your descriptive evidence from what you find in the share-based discrete choice model.\nReflect on this assignment. What did you find most challenging? What did you find most surprising?\nPlease write a couple of sentences about the course. What would you change about the class? What content would you remove or add? What did you wish you could have learned about but didn’t (or did learn about and wish you hadn’t)?"
  },
  {
    "objectID": "assignments/presentations.html",
    "href": "assignments/presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Please review the class schedule and select four papers from the list of topics in each class. Once you’ve identified your papers, note your selection and date on the Presentations tab of our shared Google Sheet (available on Canvas). Please note your selections no later than January 16, and please ensure no more than one student is signed up to present on any given day.\nYour presentation should be around 60 minutes, following a standard seminar setup. Please also prepare two discussion questions to lead a 10-15 minute discussion after your presentation. Please submit your presentations via Canvas prior to your in-class time.\nYour presentation should be no more than 50 slides (including a title slide and your discussion questions at the end). Each presentation is worth 5 points toward your final grade, with one point allocated to each of the following categories:\nNote that a presentation is not just a re-hashing of the paper in slide form. A good academic presentation should have as little text as possible on each slide, and the content on the slides doesn’t necessarily need to follow that of the paper. For example, in a real-time environment, it is much easier to move between different aspects of the empirical analysis and data. Below are lists of candidate papers for presentation for each module. These are also listed in the shared spreadsheet (on Canvas) from which you can formally indicate your selections.\nIn addition to these 4 graded presentations, I expect everyone to present and discuss recent job market papers toward the end of the semester (Module 6). While ungraded, these JMP presentations will help everyone better understand the quality and rigor of recent top job market papers in our field.",
    "crumbs": [
      "Assignments",
      "Presentations"
    ]
  },
  {
    "objectID": "assignments/presentations.html#module-1-insurance",
    "href": "assignments/presentations.html#module-1-insurance",
    "title": "Presentations",
    "section": "Module 1: Insurance",
    "text": "Module 1: Insurance\n\n\n\nPaper\nMain theme\n\n\n\n\nAron-Dine, Einav, and Finkelstein (2013)\nMoral hazard and utilization responses\n\n\nFinkelstein et al. (2012)\nMedicaid coverage and health care use\n\n\nBrot-Goldberg et al. (2023)\nChoice frictions and defaults\n\n\nHu et al. (2018)\nFinancial protection and household debt\n\n\nMiller, Johnson, and Wherry (2021)\nInsurance coverage and mortality\n\n\nFinkelstein, Hendren, and Luttmer (2019)\nWelfare effects of Medicaid\n\n\nEinav, Finkelstein, and Cullen (2010)\nEmpirical tests of adverse selection\n\n\nHandel, Kolstad, and Spinnewijn (2019)\nInertia and policy interventions\n\n\nShepard (2022)\nProvider networks and selection\n\n\nDafny, Duggan, and Ramanarayanan (2012)\nInsurer concentration and premiums\n\n\nHo and Lee (2017)\nPlan design and non-price competition\n\n\nCabral, Geruso, and Mahoney (2018)\nMedicare Advantage incentives\n\n\nCurto et al. (2021)\nCompetition in regulated insurance markets\n\n\nLayton et al. (2019)\nPublic versus private insurance provision",
    "crumbs": [
      "Assignments",
      "Presentations"
    ]
  },
  {
    "objectID": "assignments/presentations.html#module-2-physician-agency",
    "href": "assignments/presentations.html#module-2-physician-agency",
    "title": "Presentations",
    "section": "Module 2: Physician Agency",
    "text": "Module 2: Physician Agency\n\n\n\nPaper\nMain theme\n\n\n\n\nFinkelstein, Gentzkow, and Williams (2016)\nProvider incentives and patient outcomes\n\n\nBadinski et al. (2023)\nPhysician decision-making under incentives\n\n\nIizuka (2012)\nFinancial incentives and prescribing\n\n\nClemens and Gottlieb (2014)\nPhysician supply responses to payment\n\n\nHo and Pakes (2014)\nOrganizational incentives and productivity\n\n\nJ. Currie, MacLeod, and Van Parys (2016)\nPhysician behavior and treatment choice\n\n\nMolitor (2018)\nInformation frictions and treatment\n\n\nZeltzer (2020)\nReferral networks and physician behavior\n\n\nMartin Gaynor, Propper, and Seiler (2016)\nPhysician market power\n\n\nEliason et al. (2018)\nOwnership incentives and care decisions\n\n\nGruber, Hoe, and Stoye (2023)\nTreatment decisions and time constraints",
    "crumbs": [
      "Assignments",
      "Presentations"
    ]
  },
  {
    "objectID": "assignments/presentations.html#module-3-learning",
    "href": "assignments/presentations.html#module-3-learning",
    "title": "Presentations",
    "section": "Module 3: Learning",
    "text": "Module 3: Learning\n\n\n\nPaper\nMain theme\n\n\n\n\nChing, Erdem, and Keane (2013)\nBayesian learning by physicians\n\n\nChan, Narasimhan, and Xie (2013)\nDynamic learning and treatment choice\n\n\nGong (2018)\nPeer effects in learning\n\n\nComin, Skinner, and Staiger (2022)\nDiffusion of medical technologies\n\n\nCoscelli and Shum (2004)\nPhysician learning with spillovers\n\n\nCrawford and Shum (2005)\nLearning and demand dynamics\n\n\nJ. M. Currie and MacLeod (2020)\nLearning and quality improvement\n\n\nAgha and Molitor (2018)\nLearning from referrals\n\n\nDubois and Tuncel (2021)\nLearning and pharmaceutical adoption\n\n\nDickstein (2018)\nLearning and clinical practice variation",
    "crumbs": [
      "Assignments",
      "Presentations"
    ]
  },
  {
    "objectID": "assignments/presentations.html#module-4-competition",
    "href": "assignments/presentations.html#module-4-competition",
    "title": "Presentations",
    "section": "Module 4: Competition",
    "text": "Module 4: Competition\n\n\n\nPaper\nMain theme\n\n\n\n\nMartin Gaynor and Vogt (2003)\nPrice competition in hospital markets\n\n\nM. Gaynor, Ho, and Town (2015)\nReview of competition in health care\n\n\nKessler and McClellan (2000)\nCompetition and treatment outcomes\n\n\nDafny (2009)\nHospital mergers and prices\n\n\nMartin Gaynor, Moreno-Serra, and Propper (2013)\nCompetition and quality effects\n\n\nGowrisankaran, Nevo, and Town (2015)\nStructural bargaining models\n\n\nM. Lewis and Pflum (2015)\nInsurer–provider negotiations\n\n\nHo and Lee (2019)\nNetwork formation and competition\n\n\nDafny, Ho, and Lee (2019)\nCross-market mergers\n\n\nSchmitt (2018)\nMultimarket contact\n\n\nM. S. Lewis and Pflum (2017)\nHospital bargaining power\n\n\nCuesta, Noton, and Vatter (2019)\nVertical integration effects\n\n\nKoch, Wendling, and Wilson (2021)\nPhysician–hospital integration\n\n\nCapps, Dranove, and Ody (2018)\nHospital consolidation impacts",
    "crumbs": [
      "Assignments",
      "Presentations"
    ]
  },
  {
    "objectID": "assignments/presentations.html#module-5-disclosure",
    "href": "assignments/presentations.html#module-5-disclosure",
    "title": "Presentations",
    "section": "Module 5: Disclosure",
    "text": "Module 5: Disclosure\n\n\n\nPaper\nMain theme\n\n\n\n\nDranove and Jin (2010)\nQuality disclosure and incentives\n\n\nLuco (2019)\nTransparency and pricing behavior\n\n\nDranove et al. (2003)\nReport cards and provider response\n\n\nKolstad (2013)\nInformation and patient choice\n\n\nEpstein (2010)\nPublic reporting and quality\n\n\nJin and Sorensen (2006)\nDisclosure and market outcomes\n\n\nDarden and McCarthy (2015)\nInformation and provider behavior\n\n\nGrennan and Swanson (2020)\nPrice transparency and negotiation\n\n\nChristensen, Floyd, and Maffett (2020)\nDisclosure and health care prices\n\n\nBrown (2019)\nConsumer responses to transparency",
    "crumbs": [
      "Assignments",
      "Presentations"
    ]
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Here are a few additional resources that you may find helpful throughout the semester. I’ll try to add to these as we go.\n\nSome resources for honing your data analysis skills in R:\n\nR for Data Science by Hadley Wickham and Garrett Grolemund. This is the go-to resource for starting to work with R. Highly recommended, free, and informative.\nR Language Basics by Grant McDermott\nData cleaning and wrangling in R by Grant McDermott\n\nSome free econometric resources\n\nCausal Inference Book by Jamie Robin and Miguel Hernan\nEconometrics by Bruce Hansen\nIntroductory Econometrics class notes from Nick Huntington-Klein\nEconometrics Notes from Carolina Caetano at UGA\nEver wish you had a central resource for lots of little commands and how to do things in different stats packages? Nick Huntington-Klein thought the same thing in his Library of Statistical Techniques (LOST)."
  },
  {
    "objectID": "schedule/1-0.html",
    "href": "schedule/1-0.html",
    "title": "Demand for Health Insurance",
    "section": "",
    "text": "A large literature studies demand for health insurance by examining how individuals respond to coverage and how they make insurance choices under uncertainty. Experimental evidence from the RAND Health Insurance Experiment established that insurance generosity substantially affects healthcare utilization, providing early causal evidence on moral hazard and the role of cost sharing in shaping demand (Aron-Dine, Einav, and Finkelstein (2013)). More recent experimental work from the Oregon Medicaid lottery confirms that gaining insurance coverage increases utilization and improves financial security, with more nuanced effects on health outcomes (Finkelstein et al. (2012)). Together, these studies show that insurance coverage meaningfully changes behavior, while also highlighting that observed demand reflects both preferences and the structure of insurance contracts. While moral hazard and utilization responses are central to understanding insurance demand, these mechanisms are well covered elsewhere (see Einav and Finkelstein (2018) for a comprehensive treatment) and are not the primary focus here.\nBeyond responses to coverage, a growing body of work documents systematic frictions in health insurance choice. Individuals frequently select plans that are dominated given their realized healthcare needs, suggesting that limited attention, complexity, and misperceptions play an important role in insurance demand (Abaluck and Gruber (2011); Jonathan D. Ketcham et al. (2012); Jonathan D. Ketcham, Kuminoff, and Powers (2016); Abaluck and Gruber (2016)). This evidence has motivated recent research on whether low-cost policy interventions can improve insurance choices when decision-making is distorted by costly cognition. For example, default plan assignments in public drug insurance settings have been shown to strongly influence enrollment and utilization, with important implications for welfare and market design (Brot-Goldberg et al. (2023)).\nPotential papers for presentation today include:\n\nAron-Dine, Einav, and Finkelstein (2013) — RAND Health Insurance Experiment and utilization responses\nFinkelstein et al. (2012) — Oregon Medicaid lottery and the effects of insurance coverage\nBrot-Goldberg et al. (2023) — defaults, costly cognition, and insurance choice\n\n\n\n\n\nReferences\n\nAbaluck, Jason, and Jonathan Gruber. 2011. “Choice Inconsistencies Among the Elderly: Evidence from Plan Choice in the Medicare Part D Program.” American Economic Review 101 (4): 1180–210.\n\n\n———. 2016. “Choice Inconsistencies Among the Elderly: Evidence from Plan Choice in the Medicare Part D Program: Reply.” American Economic Review 106 (12): 3962–87. https://doi.org/10.1257/aer.20151318.\n\n\nAron-Dine, Aviva, Liran Einav, and Amy Finkelstein. 2013. “The RAND Health Insurance Experiment, Three Decades Later.” Journal of Economic Perspectives 27 (1): 197–222.\n\n\nBrot-Goldberg, Zarek, Timothy Layton, Boris Vabson, and Adelina Yanyue Wang. 2023. “The Behavioral Foundations of Default Effects: Theory and Evidence from Medicare Part D.” American Economic Review 113 (10): 2718–58. https://doi.org/10.1257/aer.20210013.\n\n\nEinav, Liran, and Amy Finkelstein. 2018. “Moral Hazard in Health Insurance: What We Know and How We Know It.” Journal of the European Economic Association 16 (4): 957–82. https://doi.org/10.1093/jeea/jvy017.\n\n\nFinkelstein, Amy, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan Gruber, Joseph P Newhouse, Heidi Allen, Katherine Baicker, et al. 2012. “The Oregon Health Insurance Experiment: Evidence from the First Year.” Quarterly Journal of Economics 127 (3): 1057–1106.\n\n\nKetcham, Jonathan D., Nicolai V. Kuminoff, and Christopher A. Powers. 2016. “Choice Inconsistencies Among the Elderly: Evidence from Plan Choice in the Medicare Part D Program: Comment.” American Economic Review 106 (12): 3932–61. https://doi.org/10.1257/aer.20131048.\n\n\nKetcham, Jonathan D, Claudio Lucarelli, Eugenio J Miravete, and M Christopher Roebuck. 2012. “Sinking, Swimming, or Learning to Swim in Medicare Part D.” American Economic Review 102 (6): 2639–73."
  },
  {
    "objectID": "schedule/1-2.html",
    "href": "schedule/1-2.html",
    "title": "Adverse Selection",
    "section": "",
    "text": "A central friction in health insurance markets is adverse selection, which arises when consumers with different expected healthcare costs sort into different insurance plans. When higher-risk individuals disproportionately enroll in more generous coverage, insurers may face average costs that exceed premiums, leading to premium increases, market unraveling, or the withdrawal of plans altogether. A large literature studies how adverse selection affects pricing, plan offerings, and equilibrium outcomes in health insurance markets, as well as how institutional features and policy interventions can mitigate or exacerbate these forces.\nOne strand of this literature focuses on pricing and the empirical detection of adverse selection. These papers develop and apply empirical tools to test for selection by examining the relationship between premiums, plan characteristics, and realized costs. Early and influential work shows how pricing patterns and enrollment responses can reveal the presence and magnitude of adverse selection in real-world insurance markets, providing a foundation for much of the modern empirical literature (Bundorf, Levin, and Mahoney (2012); Einav, Finkelstein, and Cullen (2010)).\nA second strand examines policy interventions aimed at mitigating adverse selection, such as mandates, subsidies, and choice architecture. This work emphasizes that while such policies can improve market stability, they may also generate unintended consequences depending on how consumers respond and how insurers adjust plan design. A series of papers demonstrates that the effectiveness of these interventions depends critically on consumer inertia, switching costs, and the interaction between policy design and market structure (B. R. Handel (2013); B. Handel, Hendel, and Whinston (2015); B. R. Handel, Kolstad, and Spinnewijn (2019)).\nMore recent work extends the analysis of adverse selection beyond premiums and benefits to consider insurance networks as a margin of selection. In this view, plan networks—particularly hospital networks—shape both consumer choice and insurer costs, introducing new channels through which adverse selection operates. Evidence from health insurance exchanges shows that network design can be used strategically to attract or deter certain types of enrollees, linking adverse selection to competition among providers and insurers in novel ways (Shepard (2022)).\nPotential papers for presentation today include\n\nEinav, Finkelstein, and Cullen (2010) — empirical tests for selection using pricing and cost data\nB. R. Handel, Kolstad, and Spinnewijn (2019) — consumer inertia, policy interventions, and adverse selection\nShepard (2022) — hospital networks as a channel for adverse selection\n\n\n\n\n\nReferences\n\nBundorf, M Kate, Jonathan Levin, and Neale Mahoney. 2012. “Pricing and Welfare in Health Plan Choice.” American Economic Review 102 (7): 3214–48.\n\n\nEinav, Liran, Amy Finkelstein, and Mark R. Cullen. 2010. “Estimating Welfare in Insurance Markets Using Variation in Prices.” The Quarterly Journal of Economics 125 (3): 877–921. https://doi.org/10.1162/qjec.2010.125.3.877.\n\n\nHandel, Ben, Igal Hendel, and Michael D. Whinston. 2015. “Equilibria in Health Exchanges: Adverse Selection Versus Reclassification Risk.” Econometrica 83 (4): 1261–1313. https://doi.org/10.3982/ECTA12480.\n\n\nHandel, Benjamin R. 2013. “Adverse Selection and Inertia in Health Insurance Markets: When Nudging Hurts.” American Economic Review 103 (7): 2643–82. https://doi.org/10.1257/aer.103.7.2643.\n\n\nHandel, Benjamin R., Jonathan T. Kolstad, and Johannes Spinnewijn. 2019. “Information Frictions and Adverse Selection: Policy Interventions in Health Insurance Markets.” Review of Economics and Statistics 101 (2): 326–40.\n\n\nShepard, Mark. 2022. “Hospital Network Competition and Adverse Selection: Evidence from the Massachusetts Health Insurance Exchange.” American Economic Review 112 (2): 578–615. https://doi.org/10.1257/aer.20201453."
  },
  {
    "objectID": "schedule/1-4.html",
    "href": "schedule/1-4.html",
    "title": "Public Insurance Design",
    "section": "",
    "text": "Public insurance programs in the U.S. increasingly rely on private insurers to deliver coverage, most notably through Medicare Advantage and Medicaid managed care. These programs are built around the idea of managed competition, in which insurers compete for enrollees under regulated payment, subsidy, and risk-adjustment schemes. A central question in this literature is whether competition in these settings benefits consumers or instead generates rents for insurers, depending on how program rules are designed.\nOne strand of this literature studies subsidy design, pass-through, and insurer incentives in public insurance markets. Empirical evidence from Medicare Advantage shows that poorly designed subsidies may be incompletely passed through to consumers, leading to higher insurer profits rather than lower premiums. These papers highlight the importance of benchmark rules, bidding incentives, and regulatory constraints in shaping equilibrium outcomes (Cabral, Geruso, and Mahoney (2018); V. Curto et al. (2021)).\nA related strand examines public versus private provision of health insurance, reflecting growing interest in the consequences of privatization within public programs. As enrollment in Medicaid managed care and Medicare Advantage has expanded, researchers have studied how private provision affects costs, access, and consumer outcomes relative to traditional public insurance. This work emphasizes tradeoffs between efficiency, selection, and administrative complexity, and remains an active area of research (Layton et al. (2019); Macambira et al. (2022)).\nFinally, recent work explores new directions in expanding access and affordability in health insurance markets, often motivated by ongoing policy debates. These papers evaluate reforms aimed at improving coverage and affordability, with a focus on welfare implications and distributional effects, and highlight open questions about how best to design insurance systems that balance access, cost, and efficiency (Geddes and Schnell (2023); V. E. Curto (2023)).\nPotential papers for presentation today include\n\nCabral, Geruso, and Mahoney (2018) — subsidies, pass-through, and insurer incentives in Medicare Advantage\nV. Curto et al. (2021) — competition and consumer outcomes in regulated insurance markets\nLayton et al. (2019) — public versus private provision in health insurance\n\n\n\n\n\nReferences\n\nCabral, Marika, Michael Geruso, and Neale Mahoney. 2018. “Do Larger Health Insurance Subsidies Benefit Patients or Producers? Evidence from Medicare Advantage.” American Economic Review 108 (8): 2048–87.\n\n\nCurto, Vilsa E. 2023. “Pricing Regulations in Individual Health Insurance: Evidence from Medigap.” Journal of Health Economics 91 (September): 102785. https://doi.org/10.1016/j.jhealeco.2023.102785.\n\n\nCurto, Vilsa, Liran Einav, Jonathan Levin, and Jay Bhattacharya. 2021. “Can Health Insurance Competition Work? Evidence from Medicare Advantage.” Journal of Political Economy 129 (2): 570–606.\n\n\nGeddes, Eilidh, and Molly Schnell. 2023. “The Expansionary and Contractionary Supply-Side Effects of Health Insurance.” Working {Paper}. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w31483.\n\n\nLayton, Timothy J., Nicole Maestas, Daniel Prinz, and Boris Vabson. 2019. “Private Vs. Public Provision of Social Insurance: Evidence from Medicaid.” Working {Paper}. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w26042.\n\n\nMacambira, Danil Agafiev, Michael Geruso, Anthony Lollo, Chima D. Ndumele, and Jacob Wallace. 2022. “The Private Provision of Public Services: Evidence from Random Assignment in Medicaid.” w30390. National Bureau of Economic Research. https://doi.org/10.3386/w30390."
  },
  {
    "objectID": "schedule/2-1.html",
    "href": "schedule/2-1.html",
    "title": "Agency and Financial Incentives",
    "section": "",
    "text": "Today we focus on the role of financial incentives in the context of physician agency problems. This literature examines whether and how physicians respond to changes in payment when making treatment decisions. Financial incentives are a natural starting point for studying agency because they provide clear, measurable variation in the marginal returns to different clinical actions. At the same time, isolating causal effects is challenging, as payment changes often coincide with broader policy reforms or shifts in patient composition.\nEarly and influential work exploits settings in which physicians directly profit from specific treatment choices. For example, Iizuka (2012) studies prescribing behavior in Japan, where physicians both prescribe and dispense drugs, creating sharp financial incentives to favor branded pharmaceuticals over generics. The paper provides clean evidence that physicians respond to these incentives, highlighting how agency can distort treatment choices even in the absence of explicit patient demand.\nMore recent work leverages quasi-experimental variation in reimbursement rates to study physician responses in the U.S. healthcare system. Clemens and Gottlieb (2014) exploits plausibly exogenous changes in Medicare physician fees to estimate supply responses, showing that physicians adjust both the volume and intensity of care in response to payment changes. Importantly, the paper also examines downstream effects on patient health, illustrating how financial incentives can influence not only utilization but outcomes.\nTogether, these studies demonstrate that physicians respond to financial incentives in systematic ways, but also raise broader questions about the effectiveness and limits of payment-based policy tools. These questions motivate later discussions of non-financial incentives and organizational constraints.\nPotential papers for presentation today include:\n\nIizuka (2012) — physician dispensing incentives and prescribing behavior\nClemens and Gottlieb (2014) — reimbursement changes and physician treatment responses\nHo and Pakes (2014) — financial incentives, hospital choice, and physician behavior\n\n\n\n\n\nReferences\n\nClemens, Jeffrey, and Joshua D Gottlieb. 2014. “Do Physicians’ Financial Incentives Affect Medical Treatment and Patient Health?” American Economic Review 104 (4): 1320–49.\n\n\nHo, Kate, and Ariel Pakes. 2014. “Hospital Choices, Hospital Prices, and Financial Incentives to Physicians.” The American Economic Review 104 (12): 3841–84.\n\n\nIizuka, Toshiaki. 2012. “Physician Agency and Adoption of Generic Pharmaceuticals.” American Economic Review 102 (6): 2826–58."
  },
  {
    "objectID": "schedule/2-3.html",
    "href": "schedule/2-3.html",
    "title": "Agency in Organization and Teams",
    "section": "",
    "text": "Much of the research on physician agency focuses on individual decision-making, holding organizational context fixed. In practice, however, physicians rarely operate in isolation. Treatment decisions are shaped by referral networks, team-based care, and organizational constraints imposed by hospitals, physician groups, and integrated delivery systems. Today, we study how physician agency operates when decision-making authority is shared across multiple providers and embedded within organizations.\nReferral networks and team structure play an important role in shaping care delivery. Patterns of interaction among physicians influence treatment intensity, coordination, and patient outcomes, even when financial incentives are held constant. Empirical studies show that referral relationships affect both where patients receive care and which treatments they receive, with implications for performance and equity in healthcare delivery (Zeltzer (2020); Agha et al. (2022)).\nPhysicians also exercise agency by influencing where patients receive care, including hospital choice and downstream referrals. In settings where patients rely on physician expertise to navigate complex provider choice sets, physician preferences and constraints can meaningfully affect hospital utilization. Evidence from institutional reforms that alter physician choice sets illustrates how agency interacts with market structure (Gaynor, Propper, and Seiler (2016)).\nFinally, agency is especially salient in settings where care is transferred or delegated across providers or facilities. These environments highlight how organizational rules and institutional incentives shape physician decision-making under tight constraints. Evidence from long-term acute care hospitals and emergency departments shows how agency affects utilization and patient outcomes in institutional settings (Eliason et al. (2018); Gruber, Hoe, and Stoye (2023)).\nTaken together, these papers show that physician agency cannot be understood solely at the individual level. Organizational structure and team-based decision-making are central to how care is delivered and how policy interventions ultimately operate. Potential papers for presentation today include:\n\nGaynor, Propper, and Seiler (2016) — physician influence on hospital choice\nEliason et al. (2018) — agency in institutional care settings\nGruber, Hoe, and Stoye (2023) - treatment decisions and time constraints\n\n\n\n\n\nReferences\n\nAgha, Leila, Keith Marzilli Ericson, Kimberley H. Geissler, and James B. Rebitzer. 2022. “Team Relationships and Performance: Evidence from Healthcare Referral Networks.” Management Science 68 (5): 3735–54. https://doi.org/10.1287/mnsc.2021.4091.\n\n\nEliason, Paul J., Paul L. E. Grieco, Ryan C. McDevitt, and James W. Roberts. 2018. “Strategic Patient Discharge: The Case of Long-Term Care Hospitals.” American Economic Review 108 (11): 3232–65. https://doi.org/10.1257/aer.20170092.\n\n\nGaynor, Martin, Carol Propper, and Stephan Seiler. 2016. “Free to Choose? Reform, Choice, and Consideration Sets in the English National Health Service.” American Economic Review 106 (11): 3521–57. https://doi.org/10.1257/aer.20121532.\n\n\nGruber, Jonathan, Thomas P. Hoe, and George Stoye. 2023. “Saving Lives by Tying Hands: The Unexpected Effects of Constraining Health Care Providers.” The Review of Economics and Statistics 105 (1): 1–19. https://doi.org/10.1162/rest_a_01044.\n\n\nZeltzer, Dan. 2020. “Gender Homophily in Referral Networks: Consequences for the Medicare Physician Earnings Gap.” American Economic Journal: Applied Economics 12 (2): 169–97. https://doi.org/10.1257/app.20180201."
  },
  {
    "objectID": "schedule/3-1.html",
    "href": "schedule/3-1.html",
    "title": "Skill Accumulation and Learning by Doing",
    "section": "",
    "text": "A distinct form of physician learning occurs through learning by doing, whereby repeated practice improves clinical skill and decision quality over time. Unlike belief updating about treatment effectiveness, skill accumulation reflects changes in a physician’s underlying productivity or ability, often tied to procedural volume and experience. This mechanism has been used to explain persistent differences in outcomes across physicians and hospitals, as well as the concentration of complex procedures among high-volume providers.\nThe literature on skill accumulation emphasizes that experience can affect both treatment choices and patient outcomes. Early empirical work documents strong volume–outcome relationships and productivity spillovers, suggesting that experience generates improvements that extend beyond the individual patient encounter (Chandra and Staiger (2007)). More recent work embeds learning-by-doing directly into dynamic models of physician behavior, allowing experience to interact with experimentation and patient selection. For example, Gong (2018) develops a structural model in which physicians simultaneously learn about treatment effectiveness and accumulate procedural skill, showing how these forces jointly shape treatment decisions and technology diffusion.\nAn important insight from this literature is that experience does not always lead to efficient learning. Skill accumulation may be distorted by biased beliefs or imperfect feedback, particularly when physicians misperceive their own ability. Comin, Skinner, and Staiger (2022) highlights how overconfidence in perceived skill can drive excessive adoption of new technologies, and how learning about this bias over time can generate both rapid diffusion and subsequent abandonment. Together, these papers underscore that learning by doing is a powerful but imperfect mechanism, with implications for productivity, technology adoption, and patient outcomes.\nPotential papers for presentation today include:\n\nGong (2018) — joint learning by doing and Bayesian updating in treatment choice\nComin, Skinner, and Staiger (2022) — skill misperception, learning, and technology adoption\n\n\n\n\n\nReferences\n\nChandra, Amitabh, and Douglas O. Staiger. 2007. “PRODUCTIVITY SPILLOVERS IN HEALTHCARE: EVIDENCE FROM THE TREATMENT OF HEART ATTACKS.” The Journal of Political Economy 115: 103–40. https://doi.org/10.1086/512249.\n\n\nComin, Diego A., Jonathan S. Skinner, and Douglas O. Staiger. 2022. “Overconfidence and Technology Adoption in Health Care.” Working {Paper}. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w30345.\n\n\nGong, Qing. 2018. “Physician Learning and Treatment Choices Evidence from Brain Aneurysms.” Working {Paper}. University of North Carolina at Chapel Hill."
  },
  {
    "objectID": "schedule/3-3.html",
    "href": "schedule/3-3.html",
    "title": "Learning from Others",
    "section": "",
    "text": "Physicians rarely learn in isolation. In addition to updating beliefs based on their own experience, doctors observe and respond to the behavior, outcomes, and information generated by others. This social learning can occur through informal peer interactions, professional networks, exposure to new clinical evidence, or high-profile early adopters. As a result, learning from others plays a central role in the diffusion—and sometimes abandonment—of medical practices and technologies.\nThe literature on social learning emphasizes that information flows across physicians can generate spillovers that amplify or dampen individual learning. Early empirical work shows that physician productivity and treatment choices respond to the experience and outcomes of nearby peers, even in the absence of direct coordination or shared incentives (Chandra and Staiger (2007)). Related work highlights the role of “pioneer” physicians and opinion leaders in shaping local adoption patterns, particularly when new technologies or drugs enter the market (Agha and Molitor (2018)). These studies demonstrate how social learning can generate clustered adoption, path dependence, and persistent regional variation in care.\nMore recent research expands the notion of learning from others to include formal sources of information, such as clinical guidelines, research trials, and performance feedback. Dubois and Tuncel (2021) shows how scientific information and recommendations influence prescribing behavior, while also documenting heterogeneity in physician responses. Together, this body of work underscores that learning from others is neither frictionless nor uniform: physicians differ in whom they observe, how they interpret signals, and how social information interacts with their own experience. This perspective provides a natural bridge to subsequent discussions of organizational structure and market forces in healthcare.\nPotential papers for presentation today include:\n\nAgha and Molitor (2018) — the role of pioneer investigators in technology adoption\nDubois and Tuncel (2021) — physician responses to scientific information and recommendations\n\n\n\n\n\nReferences\n\nAgha, Leila, and David Molitor. 2018. “The Local Influence of Pioneer Investigators on Technology Adoption: Evidence from New Cancer Drugs.” The Review of Economics and Statistics 100 (1): 29–44. https://doi.org/10.1162/REST_a_00670.\n\n\nChandra, Amitabh, and Douglas O. Staiger. 2007. “PRODUCTIVITY SPILLOVERS IN HEALTHCARE: EVIDENCE FROM THE TREATMENT OF HEART ATTACKS.” The Journal of Political Economy 115: 103–40. https://doi.org/10.1086/512249.\n\n\nDubois, Pierre, and Tuba Tuncel. 2021. “Identifying the Effects of Scientific Information and Recommendations on Physicians’ Prescribing Behavior.” Journal of Health Economics 78 (April): 102461–61. https://doi.org/10.1016/j.jhealeco.2021.102461."
  },
  {
    "objectID": "schedule/4-1.html",
    "href": "schedule/4-1.html",
    "title": "Competition in Reduced Form",
    "section": "",
    "text": "A central empirical challenge in studying hospital competition is identifying the causal effect of market structure on prices and outcomes. Early empirical work often relied on cross-sectional correlations between measures of concentration and prices, but concerns about endogeneity and market definition have motivated a shift toward research designs that exploit discrete changes in competitiveness. In hospital markets, mergers, closures, and entry events provide natural settings in which competitive conditions change sharply, allowing researchers to study how prices and quality respond.\nThe reduced-form literature on hospital mergers uses quasi-experimental designs to estimate the effects of consolidation without fully specifying the underlying pricing or bargaining process. These studies typically compare prices or outcomes in markets affected by mergers to those in unaffected markets, using difference-in-differences or event-study approaches. A central finding of this literature is that hospital mergers lead to large and persistent price increases, even in settings with nonprofit ownership and regulated payment systems. This evidence plays a prominent role in antitrust enforcement because it relies on transparent identification strategies and minimal structural assumptions.\nIn addition to prices, a smaller but important literature studies how consolidation affects hospital quality. Measuring quality responses is empirically challenging, both because quality is multidimensional and because improvements in care coordination may offset reductions in competitive pressure. Existing evidence suggests that quality effects are heterogeneous and often muted relative to price effects, with some studies finding no improvement—and in some cases deterioration—in clinical outcomes following consolidation. These results highlight an important asymmetry in merger effects and reinforce the policy relevance of price-based evidence.\nTogether, these papers motivate the need for richer models of hospital pricing and incentives, which we take up in the next class on insurer–hospital bargaining. Potential papers for presentation today include:\n\nKessler and McClellan (2000) — competition, prices, and quality in hospital markets\nDafny (2009) — design-based evidence on hospital mergers and price effects\nGaynor, Moreno-Serra, and Propper (2013) — evidence on hospital competition and patient outcomes\n\n\n\n\n\nReferences\n\nDafny, Leemore. 2009. “Estimation and Identification of Merger Effects: An Application to Hospital Mergers.” Journal of Law and Economics 52 (3): 523–50.\n\n\nGaynor, Martin, Rodrigo Moreno-Serra, and Carol Propper. 2013. “Death by Market Power: Reform, Competition, and Patient Outcomes in the National Health Service.” American Economic Journal: Economic Policy 5 (4): 134–66.\n\n\nKessler, Daniel, and Mark McClellan. 2000. “Is Hospital Competition Socially Wasteful?” Quarterly Journal of Economics 2 (115): 577–615."
  },
  {
    "objectID": "schedule/4-3.html",
    "href": "schedule/4-3.html",
    "title": "Cross-Market Mergers",
    "section": "",
    "text": "Much of the literature on hospital consolidation focuses on mergers between hospitals that compete in the same geographic market. However, a growing share of hospital mergers involve systems that operate in distinct local markets and do not directly compete for patients. These cross-market mergers pose a challenge for standard antitrust analysis, since they do not mechanically reduce local competition or increase concentration as measured by traditional market-definition tools. Understanding how such mergers affect prices therefore requires alternative economic mechanisms.\nRecent work emphasizes that cross-market consolidation can increase prices through changes in bargaining leverage rather than local market power. When a hospital system operates in multiple markets, it may gain the ability to negotiate higher prices by leveraging common insurers or employers across markets, even if hospitals within a given market remain geographically distinct. Related work draws on the concept of multimarket contact, in which firms competing across multiple markets internalize the effects of aggressive pricing in one market on competition in others. These mechanisms suggest that consolidation can raise prices even in the absence of within-market overlap.\nWe introduce this literature by focusing on empirical studies that test these mechanisms in hospital markets. These papers examine how prices respond to system expansion across markets and how shared customers and multimarket interactions shape bargaining outcomes. Together, they extend the standard merger framework and highlight the importance of system-level organization for understanding hospital pricing.\nPotential papers for presentation today include:\n\nDafny, Ho, and Lee (2019) — evidence on cross-market mergers and the common-customer mechanism\nSchmitt (2018) — multimarket contact and hospital pricing\nLewis and Pflum (2017) — system expansion and bargaining leverage across markets\n\n\n\n\n\nReferences\n\nDafny, Leemore, Kate Ho, and Robin S Lee. 2019. “The Price Effects of Cross-Market Mergers: Theory and Evidence from the Hospital Industry.” RAND Journal of Economics 50 (2): 286–325.\n\n\nLewis, Matthew S, and Kevin E Pflum. 2017. “Hospital Systems and Bargaining Power: Evidence from Out-of-Market Acquisitions.” The RAND Journal of Economics 48 (3): 579–610.\n\n\nSchmitt, Matt. 2018. “Multimarket Contact in the Hospital Industry.” American Economic Journal: Economic Policy 10 (3): 361–87."
  },
  {
    "objectID": "schedule/5-0.html",
    "href": "schedule/5-0.html",
    "title": "Basics of Information Disclosure",
    "section": "",
    "text": "A defining feature of healthcare markets is that key dimensions of quality, prices, and performance are often difficult for patients, providers, and payers to observe. These information frictions motivate a wide range of disclosure policies, including public reporting of provider quality, insurance plan ratings, and posted prices. Understanding how disclosure affects behavior requires careful attention to who receives the information, what exactly is disclosed, and how agents incorporate that information into their decisions.\nThe literature on information disclosure studies how increased observability changes market outcomes through several channels. On the demand side, disclosure can shift patient or enrollee choice by reducing uncertainty or correcting misperceptions. On the supply side, disclosure can alter provider incentives through reputation, referral patterns, and competitive pressure. At the same time, disclosure may induce strategic responses that complicate both empirical measurement and welfare analysis.\nWe introduce this literature by focusing on core economic mechanisms and general lessons that apply across institutional settings. This framework provides the foundation for the next classes on physician quality reporting, insurance ratings, and price transparency. Potential papers for presentation today include:\n\nDranove and Jin (2010) — a canonical survey of theory and evidence on quality disclosure and certification\nLuco (2019) — modern evidence on equilibrium and distributional effects of price disclosure\n\n\n\n\n\nReferences\n\nDranove, David, and Ginger Zhe Jin. 2010. “Quality Disclosure and Certification: Theory and Practice.” Journal of Economic Literature 48 (4): 935–63.\n\n\nLuco, Fernando. 2019. “Who Benefits from Information Disclosure? The Case of Retail Gasoline.” American Economic Journal: Microeconomics 11 (2): 277–305. https://doi.org/10.1257/mic.20170110."
  },
  {
    "objectID": "schedule/5-2.html",
    "href": "schedule/5-2.html",
    "title": "Insurance Ratings",
    "section": "",
    "text": "In insurance markets, consumers typically observe prices but face substantial uncertainty about plan quality, including network adequacy, customer service, and utilization management. Public ratings systems—such as Medicare Advantage star ratings—are designed to summarize these multidimensional quality attributes and guide consumer choice. Because these ratings are also tied to regulatory and financial incentives, they have the potential to shape both demand and insurer behavior.\nThe literature on insurance ratings studies how consumers respond to publicly reported plan quality and how insurers adjust plan design, marketing, and coding practices in response to rating thresholds. On the demand side, ratings can reallocate enrollment across plans, affecting market shares and competitive dynamics. On the supply side, ratings may induce strategic responses that improve measured performance without necessarily improving underlying quality. As a result, insurance ratings provide a useful setting for studying the interaction between information disclosure, incentives, and market design.\nWe introduce this literature by focusing on empirical work that identifies the causal effects of insurance ratings on enrollment and insurer behavior. These papers illustrate how disclosure can amplify or distort incentives when ratings are embedded in regulatory frameworks, and they provide a natural bridge to broader discussions of transparency and competition in health insurance markets.\nPotential papers for presentation today include:\n\nJin and Sorensen (2006) — a classic analysis of how publicized plan ratings affect consumer choice\nDarden and McCarthy (2015) — evidence on the impact of Medicare Advantage star ratings on enrollment\n\n\n\n\n\nReferences\n\nDarden, M., and I. McCarthy. 2015. “The Star Treatment: Estimating the Impact of Star Ratings on Medicare Advantage Enrollments.” Journal of Human Resources 50 (4): 980–1008.\n\n\nJin, G. Z., and A. T. Sorensen. 2006. “Information and Consumer Choice: The Value of Publicized Health Plan Ratings.” Journal of Health Economics 25 (2): 248–75."
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": "Schedule",
    "section": "",
    "text": "Below is a breakdown of topics for each class day this semester. Assignments are also listed according to their due dates. The associated links take you to additional information relevant for that day or for that assignment. Each heading is also linked to a broader literature review for that topic, containing many more papers than we will cover in class. For those of you interested in learning more about a particular topic, this is a good place to start."
  },
  {
    "objectID": "schedule/index.html#welcome-to-the-class",
    "href": "schedule/index.html#welcome-to-the-class",
    "title": "Schedule",
    "section": "Welcome to the class!",
    "text": "Welcome to the class!\n\n\n\n\n\n\n\n\n\nClass\nDates\nTopic\nDescription\n\n\n\n\n1\n1/13\nIntroductions"
  },
  {
    "objectID": "schedule/index.html#module-1-health-insurance-and-market-design",
    "href": "schedule/index.html#module-1-health-insurance-and-market-design",
    "title": "Schedule",
    "section": "Module 1: Health Insurance and Market Design",
    "text": "Module 1: Health Insurance and Market Design\n\n\n\n\n\n\n\n\n\nClass\nDates\nTopic\nDescription\n\n\n\n\n2\n1/15\nDemand for Health Insurance\n\n\n\n\n1/16\nSelect Papers to Present\n\n\n\n3\n1/20\nWelfare Effects of Health Insurance\n\n\n\n4\n1/22\nAdverse Selection\n\n\n\n5\n1/27\nCompetition and Market Structure\n\n\n\n6\n1/29\nPublic Insurance Design\n\n\n\n\n\nSelect Empirical Exercise"
  },
  {
    "objectID": "schedule/index.html#module-2-physician-agency-and-treatment-decisions",
    "href": "schedule/index.html#module-2-physician-agency-and-treatment-decisions",
    "title": "Schedule",
    "section": "Module 2: Physician Agency and Treatment Decisions",
    "text": "Module 2: Physician Agency and Treatment Decisions\n\n\n\n\n\n\n\n\n\nClass\nDates\nTopic\nDescription\n\n\n\n\n7\n2/3\nPhysician Agency\n\n\n\n8\n2/5\nFinancial Incentives\n\n\n\n\n\nResearch Proposal 1 Due\n\n\n\n9\n2/10\nNon-financial Incentives\n\n\n\n10\n2/12\nAgency in Organizations and Teams"
  },
  {
    "objectID": "schedule/index.html#module-3-physician-learning-and-technology-adoption",
    "href": "schedule/index.html#module-3-physician-learning-and-technology-adoption",
    "title": "Schedule",
    "section": "Module 3: Physician Learning and Technology Adoption",
    "text": "Module 3: Physician Learning and Technology Adoption\n\n\n\n\n\n\n\n\n\nClass\nDates\nTopic\nDescription\n\n\n\n\n11\n2/17\nLearning under Uncertainty\n\n\n\n12\n2/19\nSkill Accumulation\n\n\n\n13\n2/24\nPatient-Treatment Match Value\n\n\n\n14\n2/26\nLearning from Others\n\n\n\n\n\nResearch Proposal 2 Due"
  },
  {
    "objectID": "schedule/index.html#module-4-hospital-competition-and-pricing",
    "href": "schedule/index.html#module-4-hospital-competition-and-pricing",
    "title": "Schedule",
    "section": "Module 4: Hospital Competition and Pricing",
    "text": "Module 4: Hospital Competition and Pricing\n\n\n\n\n\n\n\n\n\nClass\nDates\nTopic\nDescription\n\n\n\n\n15\n3/3\nUnderstanding Hospital Competition\n\n\n\n16\n3/5\nCompetition in Reduced Form\n\n\n\n17\n3/17\nBargaining and Hospital Pricing\n\n\n\n\n\nExcellence Blueprint Due\n\n\n\n18\n3/19\nCross-market Mergers\n\n\n\n19\n3/24\nVertical Integration"
  },
  {
    "objectID": "schedule/index.html#module-5-information-disclosure-and-transparency",
    "href": "schedule/index.html#module-5-information-disclosure-and-transparency",
    "title": "Schedule",
    "section": "Module 5: Information Disclosure and Transparency",
    "text": "Module 5: Information Disclosure and Transparency\n\n\n\n\n\n\n\n\n\nClass\nDates\nTopic\nDescription\n\n\n\n\n20\n3/26\nBasics of Information Disclosure\n\n\n\n\n\nResearch Proposal 3 Due\n\n\n\n21\n3/31\nPhysician Quality\n\n\n\n22\n4/2\nInsurance Ratings\n\n\n\n23\n4/7\nPrice Transparency"
  },
  {
    "objectID": "schedule/index.html#module-6-recent-job-market-papers",
    "href": "schedule/index.html#module-6-recent-job-market-papers",
    "title": "Schedule",
    "section": "Module 6: Recent Job Market Papers",
    "text": "Module 6: Recent Job Market Papers\n\n\n\n\n\n\n\n\n\nClass\nDates\nTopic\nDescription\n\n\n\n\n24\n4/9\nJMPs from 2023\n\n\n\n25\n4/14\nJMPs from 2024\n\n\n\n26\n4/16\nJMPs from 2025"
  },
  {
    "objectID": "schedule/index.html#final-research-plans",
    "href": "schedule/index.html#final-research-plans",
    "title": "Schedule",
    "section": "Final Research Plans",
    "text": "Final Research Plans\n\n\n\n\n\n\n\n\n\nClass\nDates\nTopic\nDescription\n\n\n\n\n27\n4/21\nResearch Plans\n\n\n\n28\n4/23\nResearch Plans\n\n\n\n\n\nEmpirical Exercise Due"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course explores the industrial organization of healthcare markets in the U.S., sometimes referred to as supply-side health economics or the economics of healthcare (to differentiate from the economics of health). We will focus on the following areas: health insurance, physician agency, physician learning, healthcare pricing and competition, and information disclosure. As we cover some of the key papers in these areas, we will discuss and employ tools from the fields of empirical IO and causal inference. These methods will be discuseed as needed throughout the course, but students are expected to have a working knowledge of these methods prior to the start of the course."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "This course explores the industrial organization of healthcare markets in the U.S., sometimes referred to as supply-side health economics or the economics of healthcare (to differentiate from the economics of health). We will focus on the following areas: health insurance, physician agency, physician learning, healthcare pricing and competition, and information disclosure. As we cover some of the key papers in these areas, we will discuss and employ tools from the fields of empirical IO and causal inference. These methods will be discuseed as needed throughout the course, but students are expected to have a working knowledge of these methods prior to the start of the course."
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nI have four central goals for this course:\n\nSynthesize the current literature in each of the main areas of health economics covered in this class\nApply standard causal inference techniques in the area of healthcare\nCritically evaluate key papers in the field, identifying core strengths of the research\nDevelop your own preliminary research in some area of healthcare economics\n\nOur class times and your presentations are designed to help achieve the first goal; the second and third goals align with our homework assignments; and the final goal is achieved through the research proposals and research plan. The assignments are described in more detail on the class assignments page."
  },
  {
    "objectID": "syllabus.html#text-software-and-class-materials",
    "href": "syllabus.html#text-software-and-class-materials",
    "title": "Syllabus",
    "section": "Text, Software, and Class Materials",
    "text": "Text, Software, and Class Materials\n\nReadings: As an elective PhD course, we will rely on academic papers from the reading list in each module. I expect everyone to read the papers in advance and come to class with questions on the study’s contribution, empirical techniques, identification strategies, and datasets used. My goal with each paper is to discuss the analysis in as much detail as possible within our time constraints. As such, we’ll focus on relatively fewer papers in class. I’ve provided a more comprehensive reading list in each module for those interested in additional readings in a specific area.\nSoftware: For anything data related, I’ll use R, but you are free to use whatever software you’re most comfortable with in your empirical work. I encourage you to use R, Stata, or Python simply because these are the most common programs used in practice right now. You’ll also need to have a basic working knowledge of Git and GitHub. If you’re new to these tools, take a look at Grant McDermott’s notes on Data Science for Economists as well as Jenny Bryan’s online reference book, Happy Git and GitHub with R.\nSlides and Notes: All presentations will be made available on our class website or Canvas."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\nVarious policies for this course are described below. Basically, let’s all work to be good citizens and take seriously our various roles as a student, teacher, friend, colleague, human, etc.\n\nAcademic integrity\nThe Emory University Honor Code is taken seriously and governs all work in this course. Details about the Honor Code are available in the Laney Graduate School Handbook and available online here. By taking this course, you affirm that it is a violation of the code to plagiarize, to deviate from the instructions about collaboration on work that is submitted for grades, to give false information to a faculty member, and to undertake any other form of academic misconduct. You also affirm that if you witness others violating the code you have a duty to report them to the honor council.\n\n\nAccessibility services\nIf you anticipate issues related to the format or requirements of this course, please meet with me. I would like us to discuss ways to ensure your full participation in the course. If you determine that accommodations are necessary, you may register with Accessibility Services at (404)727-9877 or via e-mail at accessibility@emory.edu. To register with OAS, students must self identify and initiate contact with the OAS office.\n\n\nCommunication\nI will post regular announcements to the class on Canvas, so please set up your notifications accordingly. I will also use Canvas to post all grades and any other information that needs to stay in the class (like our Zoom meeting link for virtual meetings, if needed). All other course materials will be available on our class website. Please feel free to reach out to me for any reason. I generally respond to all e-mails within 24 hours.\n\n\nOffice Hours\nMy designated office hours are 1:00-2:30pm on Tuesday and Thursday in R418 of the R. Randall Rollins building. I’m happy to meet outside of normal office hours as well. Please feel free to schedule another time to meet by following the link to select a time that works for you, https://mccarthy-meetings.youcanbook.me. Unless otherwise noted, all meetings will be held in my office. If you need to meet virtually, please let me know and I’ll send you a Zoom link.\n\n\nAttendance\nWhile there is no official “attendance” credit, everyone is expected to attend all class sessions. Given our small class, it is very important that we are all present and engaged."
  },
  {
    "objectID": "syllabus.html#assignments-and-grading",
    "href": "syllabus.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nThere are four main assignments throughout the semester. I describe each assignment below, with more detail provided on our assignments page.\n\nResearch Proposals and Final Plans\nYou will submit three research proposals throughout the semester. These are short, 1-2 page documents that outline a problem/motivation, a related research question, potential data to help answer this question, and a proposed empirical strategy. The goal of these proposals is to help you develop your own research ideas and to get feedback from me and your classmates.\nToward the end of the semester, I will meet with each of you (one-on-one) to discuss your proposals. Based on that feedback, you will develop one of these proposals into a more complete research plan. Details of the research proposal and plans are available on the assignments page of our class website.\n\n\nPresentations\nYou will present four papers throughout the course of the semester. Please note your selected papers and class dates on the presentations tab of our shared Google Sheet no later than Friday, January 16. If you do not select papers by then, I’ll have to assign papers unilaterally. The complete list of potential papers to present and details of the expectations for each presentation are available on the assignments page of our class website. Please look to the class schedule for a list of potential papers for each class day.\n\n\nEmpirical Exercises\nThere is an applied component of this course where we spend some time with a real-life causal inference question. Raw data for each exercise will be provided on our class OneDrive notebook, the link to which is on Canvas, or simply through the paper’s replication package online. There are four possible empirical exercises, from which you must choose one. Please note your selections on the exercises tab of our shared Google Sheet no later than Thursday, January 29.\n\n\nExcellence Blueprint\nRather than writing a referee-style critique that emphasizes flaws, this assignment asks you to analyze what makes a published paper successful and influential. Your report should explain the specific research design choices and writing decisions that justify the paper’s publication and reputation, focusing on both the scientific contribution (question, identification, and fit in the literature) and the art of execution (motivation, structure, clarity, and policy relevance). The goal is to understand how strong papers persuade readers and advance the field—not how they could have been rejected. Details are avilable on the assignments page of our class website. You can submit your excellence blueprint for one of the four papers that you select to present.\n\n\nImportant Deadlines\nThis section is just to highlight important dates for assignments throughout the semester. Note that late assignments will receive an automatic 5% reduction in the grade for each day the assignment is turned in after the due date.\n\nSelect Papers to Present no later than January 16: Note the papers you plan to present on the presentations tab of our shared Google Sheet (link available on Canvas). Only one student per paper, so this is first-come first-served. Remember you need to select four papers to present throughout the semester. Please spread these presentations out over the semester.\nSelect Empirical Exercise no later than January 29: Note your selection on the exercises tab of our shared Google Sheet. No more than two students per exercise.\nThe first Research Proposal is due on February 5\nThe second Research Proposal is due on February 26\nThe Excellence Blueprint is due on March 17\nThe third Research Proposal is due on March 26\nFinal Research Plans are due on April 21 and will be presented in class on April 21 and April 23\n\nEmpirical Exercises are due on April 23"
  },
  {
    "objectID": "syllabus.html#final-grades",
    "href": "syllabus.html#final-grades",
    "title": "Syllabus",
    "section": "Final grades",
    "text": "Final grades\n\n40% for research proposals (10% for each proposal and 10% for the research plan)\n20% for empirical exercises\n20% for excellence blueprint\n20% for presentations of selected papers (5% each)\n\nLetter grades will be assigned at the end of the course based on total score achieved: (A = 100-93%, A- = 92.99-90%, B+ = 89.99-87%, B = 86.99-83%, B- = 82.99-80%, C+ = 79.99-77%, C = 76.99-73%, C- = 72.99-70%, D+ = 69.99-67%, D = 66.99-60%, F = 60% or less)"
  },
  {
    "objectID": "schedule/1-literature.html",
    "href": "schedule/1-literature.html",
    "title": "Health Insurance",
    "section": "",
    "text": "Module 1 covers health insurance markets. We begin with demand for health insurance and the welfare consequences of coverage, establishing how risk, prices, and moral hazard shape utilization and consumer welfare. We then study adverse selection as a central friction in insurance markets, before turning to insurer competition and market structure. The module concludes with public insurance design, emphasizing how government programs interact with private markets and how policy design affects incentives, enrollment, and efficiency. The topic of “health insurance” is extremely broad, but nonetheless, here’s a long list of papers that I think are particularly relevant in this area.\n\n    List of Key Papers (click to expand)"
  },
  {
    "objectID": "schedule/3-literature.html",
    "href": "schedule/3-literature.html",
    "title": "Physician Learning",
    "section": "",
    "text": "Module 3 covers physician learning. More specifically, we study how physicians learn from experience, information, and peers, and how that learning shapes treatment choices and the adoption of new medical technologies. The readings span foundational models of learning under uncertainty, empirical evidence on learning-by-doing and skill accumulation, and work on learning about patient–treatment matches. The module also highlights the role of social learning and peer effects in the diffusion and abandonment of technologies. Together, these papers emphasize learning as a central mechanism driving heterogeneity in physician behavior, treatment patterns, and productivity, complementing the static incentive-based frameworks introduced earlier in the course.\n\n    List of Key Papers (click to expand)"
  },
  {
    "objectID": "schedule/5-literature.html",
    "href": "schedule/5-literature.html",
    "title": "Information Disclosure",
    "section": "",
    "text": "Module 5 considers information disclosure as a central policy and market-design tool in healthcare. The papers (listed in the drop-down below) examine how disclosing information about quality, prices, or performance affects behavior by patients, providers, insurers, and regulators. A recurring theme is that disclosure operates through multiple channels—consumer choice, provider incentives, learning, and strategic response—and its effects depend critically on what information is disclosed, to whom, and in what market environment. The readings span foundational theoretical and empirical work on report cards and ratings, as well as more recent evidence on insurance ratings and price transparency, highlighting both intended and unintended consequences of transparency policies.\n\n    List of Key Papers (click to expand)"
  }
]